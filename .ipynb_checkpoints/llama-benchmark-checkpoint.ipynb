{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a369b991-4d1c-4ea1-b5a5-3090d526e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Clone the model to create a modified version\u001b[39;00m\n\u001b[1;32m    177\u001b[0m modified_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m replace_attention_modules(modified_model)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Use GPU if available\u001b[39;00m\n\u001b[1;32m    181\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m, in \u001b[0;36mreplace_attention_modules\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    123\u001b[0m modified_attention \u001b[38;5;241m=\u001b[39m ModifiedLlamaAttention(module\u001b[38;5;241m.\u001b[39mconfig, module\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    124\u001b[0m modified_attention\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 125\u001b[0m modified_attention\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mq_proj\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    126\u001b[0m modified_attention\u001b[38;5;241m.\u001b[39mk_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mk_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    127\u001b[0m modified_attention\u001b[38;5;241m.\u001b[39mk_proj\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mk_proj\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from fasta import *\n",
    "from typing import *\n",
    "\n",
    "class ModifiedLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = fasta_attn(query_states, key_states.transpose(2, 3),128) / math.sqrt(self.head_dim)\n",
    "        # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# Replace LlamaAttention with ModifiedLlamaAttention in the model\n",
    "def replace_attention_modules(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and hasattr(module, \"q_proj\"):\n",
    "            # Get parent module and replace the attention module\n",
    "            parent_name = name.rsplit(\".\", 1)[0]\n",
    "            parent = dict(model.named_modules())[parent_name]\n",
    "\n",
    "            # Instantiate ModifiedLlamaAttention and copy weights\n",
    "            modified_attention = ModifiedLlamaAttention(module.config, module.layer_idx)\n",
    "\n",
    "            # Copy weights and biases for q_proj\n",
    "            modified_attention.q_proj.weight.data = module.q_proj.weight.data.clone()\n",
    "            if module.q_proj.bias is not None:\n",
    "                modified_attention.q_proj.bias.data = module.q_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for k_proj\n",
    "            modified_attention.k_proj.weight.data = module.k_proj.weight.data.clone()\n",
    "            if module.k_proj.bias is not None:\n",
    "                modified_attention.k_proj.bias.data = module.k_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for v_proj\n",
    "            modified_attention.v_proj.weight.data = module.v_proj.weight.data.clone()\n",
    "            if module.v_proj.bias is not None:\n",
    "                modified_attention.v_proj.bias.data = module.v_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for o_proj\n",
    "            modified_attention.o_proj.weight.data = module.o_proj.weight.data.clone()\n",
    "            if module.o_proj.bias is not None:\n",
    "                modified_attention.o_proj.bias.data = module.o_proj.bias.data.clone()\n",
    "\n",
    "            # Replace the module\n",
    "            setattr(parent, name.split(\".\")[-1], modified_attention)\n",
    "\n",
    "            print(f\"Replaced {name} with ModifiedLlamaAttention\")\n",
    "\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(model, tokenizer, text, device):\n",
    "    \"\"\"\n",
    "    Computes the perplexity of the input text and returns the model's output.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        text (str): The input text.\n",
    "        device (torch.device): The device to run the computation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (perplexity, model_output_text)\n",
    "    \"\"\"\n",
    "    # Tokenize input text and move tensors to the specified device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)  \n",
    "    model.to(device)  # Move model to the specified device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass with labels for computing loss\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    \n",
    "    # Compute perplexity from loss\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    # Generate output text using the model's logits\n",
    "    logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # Get token IDs with highest probability\n",
    "    model_output_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return perplexity.item(), model_output_text\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Clone the model to create a modified version\n",
    "modified_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "replace_attention_modules(modified_model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample text\n",
    "text = open(\"sample_data.txt\",\"r\").read()\n",
    "# Measure original model performance\n",
    "start_time = time.time()\n",
    "original_perplexity,output = compute_perplexity(model, tokenizer, text, device)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "# Measure modified model performance\n",
    "start_time = time.time()\n",
    "modified_perplexity,output_modified = compute_perplexity(modified_model, tokenizer, text, device)\n",
    "modified_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Perplexity: {original_perplexity}, Time: {original_time:.4f}s Output: {output}\")\n",
    "print(f\"Modified Perplexity: {modified_perplexity}, Time: {modified_time:.4f}s Output: {output_modified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8fb3c-17b4-4f25-84ac-2c2e9384236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a3360-fb8d-4a0a-9779-a26d6b9e56fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
