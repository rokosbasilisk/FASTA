{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a369b991-4d1c-4ea1-b5a5-3090d526e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Perplexity: 4.602029323577881, Time: 2.7921s Output: Questioning: a staple new that the learning. and developed by   paper byLearning is All You Need\" by Ashwani et al. in 2017. In paper has been been a de of many state in deep language processing (NLP) and machine fields. including computer vision, speech learning, and robotics quantum folding. In paper idea of Transformers lies in their ability to capture the-range dependencies in a, self-attention mechanisms. which the need for explicit neural convolutional layers.In What Transformer Architecture\n",
      "\n",
      "The its core, the transformer architecture consists of three encoder andcoder pair, The the encoder and decoder are composed of self layers, each of a main components: the-attention and and feed-forward networks networks.FFNss The self takes the input sequence, generates a sequence of intermediate, while the decoder reconstruct the output sequence based on these representations.\n",
      "\n",
      " the outputs.\n",
      "\n",
      "The Self-Attention Mechanisms\n",
      "\n",
      "The-attention is a core component of enables transformers transformer its name. It allows the model to learn weights weights of importance to different parts of the input sequence, making the. This is achieved through a use of attention weights between each of tokens in The, the token in the input is represented to a-dimensional: aquery**,Q)**, **key (K)**, and **value (V)**. The attention score are computed as follows dot product of these and K, and by a softmax root of the respectiveality. and by a softmax operation to The resulting scores are then used to the value vector to generate the attention representationTheematically, the-attention can be expressed as:\n",
      "\n",
      "$$begin\\begin{Attention}(Q, K, V) = \\text{softmax}left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
      "\\]\n",
      "\n",
      "where Feed-Head Attention\n",
      "\n",
      "The further the model's ability to process long information between the introduce multi-head attention. This is splitting the input, K, and V vectors into multiple headsheads\" and applying separate-attention to to each head The resulting of each heads are then combined and fedly projected to produce the final representation.\n",
      "\n",
      " This-head attention can the model to capture on different aspects of the input sequence,Math Transformeral Encoding\n",
      "\n",
      "Position traditional or, transformers do the memory information. To address this, positional encodingodings are introduced to the input sequence. provide a about the position and These encodings are be either or learned, are typically represented as sinusoidal functions.### Feed and In\n",
      "\n",
      "Trainingers are trained using apropagation through gradient descent. with with a-scale language and The eachLP tasks, the model is typically trained on a sequence-level loss-entropy loss, For components include such as masking use optimizer and dropout rate schedulingup,, are been effective for achieving training.\n",
      "\n",
      "### transformer of transformers-trained and fin-tuning hasigms has further expanded the performance of transformers. Pre trained BERT andBidirectional Encoder Representations from Transformers) and GPT-Generative Pre-trained Transformer) have-trainedain on large amountsa and selfupervised learning, such as masked language modeling and nextoregressive inference generation. to then then fine-tuned for downstream downstream tasks.\n",
      "\n",
      "### Applications\n",
      "\n",
      " Transformers\n",
      "\n",
      "Transformers have found impressive success in a wide of domains,•. NaturalNatural Language Processing**: Transformers like BERT and GPT, and XL5 have theLP tasks, enablingelling in tasks such as machine, questionization, question question answeringanswering.2. **Computer Vision**: Transformers models haveViT) have achieved transformers-based to image recognition, object detection, and semantic tasks3. **Re Recognition**: Transformers have speech for Wave and speech recognition and TNet for audio synthesis.\n",
      "4. **Textific Computing**: Transformers have been used in fields folding,PFold), drug (Chemolecular dynamics prediction), and medical (quantolving differentialDEs).\n",
      "### Limit and Limit Directions\n",
      "\n",
      "While their impressive, transformers face several such including as long computational costs, limited requirements. and for large sequences. Further is efficient architectures architectures, such as sparseformer and hasBird, and Bformer, has to address these issues. reducing the number complexity of self-attention.\n",
      "\n",
      "Transform, theability and a challenge, While transformers scores provide valuable insights into the behavior, they the underlyingplay between different weights and the is challenging open challenge of research.\n",
      "\n",
      " Additionally challenge is the development of transformers to newodal data, where information and images, and audio data.\n",
      "\n",
      " a unified framework.\n",
      "\n",
      "### conclusion, transformers have revolutionaped the landscape of natural learning, revolution a tools for natural complex problems in As the field continues, we advancements and transformer, interpretability, and multimodal integration will expected to propel even hor for pushingifying transformers' position as the transformative of modern AI.\n",
      "Modified Perplexity: 1109411.375, Time: 2.6888s Output: defdeindededededededeworkdedededemymyworkmymyWINWINWINWINWINmyWINWINWINWINmyoneonemymymymymyWINWINoneonemyonemymyoneoneonemyonemymymymyotherstarsenonemystarsenmymyenenstarsstarsstarsenenmystarsenstarsenenstarsstarsenenenstarsstarsstarsstarsenstarsenstarsenenenenenstarsenenstarsenenenstarsenstarsenenenstarsstarsenenstarsenenenenenenenenenenenenenenenenenenenenenstarsstarsenenenstarsenenenenstarsenenenenenenenenenenenenenenazonenenenenenenenenazonenenenenenenenenenenenlabelenenenenenenenenenenenazonenenlabelenenenazonenenenenanalenazonenazonenانوenazonoutilenenanalenenazonenenenenazonenenanalenanalenenanalenazonenazonenazonazonenenenenanalenanalanallabelazonenenazonazonenazonenenenazonenen=labelazonenenazonenazon=labelazonazonazonazonazonazonazonenazonenazonenazonazonazonazonazonenazonanalazonen Accomazonazon=labelazonenazonazonazonazon=labelazonazonazonazon versaazonazonazonanalazon versaazonazonenazonazonanalazon versaazonazonazonazonazonazonanalazon=labelazonazonazonazonazonazonazon versa versaanal versaazon versaazon versa versa versaazonanalarendanal versa versa versa versaanalazonazon versaazonarendazon versaarendazonazonazonazonarendarendazon=labelazonanalazonazon versaarend versaazonarendazonazonazonazonarend versaazonarendazon versaazonarend versaarendarend versa versa versaazonarendazonarendarendarendarendazon versa=label versaarend versaazon versaanal versaazonarend versa versaarendarendarend versa versaarend versaarend versa versaazonazonarend來 versaarend versa versa versaazonarend versa versa versa versaarend versaarendarendazonarend versaazonarendarend versa versa versa versaarend versa versaazonarendarendarendazonanalarendazon versa versa versaanal versaarendarendarend versaarend versa versaazonanal versa versa versa versa versa versa versa versaarendarend versaarend versaarend versa=label versa versaarend versa versa versa versaarend versaarend versa versaanal versaarend versa versaarend versa versa versa來 versa versa=labelarend versaarend versa versa versa versa versa versaarend versa versa versa versa versa versa versaarendarend versa versa versa versaarend versaarend versa versaarendarend versa versa versa來 versa versaarend versa versa versaanal來arend versa versa versa versa versa versaarendarendarend versa versa versa versa versaarend versa versa versaarend versa versa versa versa versa versa versa versaarend versa versaarend versa versa versa versa versa versa versa versaarend versa versaazon versaarendarendarend versaarendarendarend versa versa versaarend versa versaarendarend versaarendarend versa versa versa versa versaarend versa versa versa versaarend versa versa versa versaarend versa versa versa versaarend versa versa versaarend versa versaarend versa versaarend versa versaarendarendarend versa versa versaarend versa versaarend versa versa versa versa versa versaarendarendarend versa versa versaarend versa versaarend versa versaarend versa versa versa versa versa versa versaarend versaarend versa versa versa versa versa versa versa versa versa versaarend versaarend versa versaarendarendarend versaarend versaarend versaarend versaarend versaarend versa versa versaarend versa versa來 versaarend versa versa versa來 versaarend versaarend versaarend versa versa versa versa versa versa versaarend versaarendarend versa versa versa versaarend versa versa versaarend來 versa versa versa versa versa versa versa versa versa versa versa versa versa來 versaarend versa versa versaarend versa versaarendarendarend versa versaarend versa versa versa versaarendarend來 versa versa versaarend versa versa versa versaarendarend versa versa versaarend versa versaarend versaarend versaarendarend versa versa versa versaarend versa versa來 versa versa versa versa versa versa versaarendarend versaarend versaarend versa versa versaarend versa versa versa versaarendarendarendarend versa versa來 versa versa versa versa versa versa versa versaarend versaarend versa versa versa versa versaarend versa versa versa versa versa versaarend versa versaarend versaarend versa versa versa versa versa versaarend versa versa versaarend versa versa versaarend versa versa versa versa versaarendarendarend versa來 versaarend versa versa versa versaarendarendarend versa versa versaarend versaarend versa versa versa versa versa versa versaarend versaarend versa versaarend versa versa versaarendarend versa versa versa versaarend versa versa\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from fasta import *\n",
    "from typing import *\n",
    "\n",
    "class ModifiedLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = fasta_attn(query_states, key_states.transpose(2, 3),128) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# Replace LlamaAttention with ModifiedLlamaAttention in the model\n",
    "def replace_attention_modules(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and hasattr(module, \"q_proj\"):\n",
    "            parent_name = name.rsplit(\".\", 1)[0]\n",
    "            parent = dict(model.named_modules())[parent_name]\n",
    "            setattr(parent, name.split(\".\")[-1], ModifiedLlamaAttention(module.config, module.layer_idx))\n",
    "\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(model, tokenizer, text, device):\n",
    "    \"\"\"\n",
    "    Computes the perplexity of the input text and returns the model's output.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        text (str): The input text.\n",
    "        device (torch.device): The device to run the computation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (perplexity, model_output_text)\n",
    "    \"\"\"\n",
    "    # Tokenize input text and move tensors to the specified device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)  \n",
    "    model.to(device)  # Move model to the specified device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass with labels for computing loss\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    \n",
    "    # Compute perplexity from loss\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    # Generate output text using the model's logits\n",
    "    logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # Get token IDs with highest probability\n",
    "    model_output_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return perplexity.item(), model_output_text\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Clone the model to create a modified version\n",
    "modified_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "replace_attention_modules(modified_model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample text\n",
    "text = open(\"sample_data.txt\",\"r\").read()\n",
    "# Measure original model performance\n",
    "start_time = time.time()\n",
    "original_perplexity,output = compute_perplexity(model, tokenizer, text, device)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "# Measure modified model performance\n",
    "start_time = time.time()\n",
    "modified_perplexity,output_modified = compute_perplexity(modified_model, tokenizer, text, device)\n",
    "modified_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Perplexity: {original_perplexity}, Time: {original_time:.4f}s Output: {output}\")\n",
    "print(f\"Modified Perplexity: {modified_perplexity}, Time: {modified_time:.4f}s Output: {output_modified}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
