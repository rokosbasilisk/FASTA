{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda12cd-d349-4088-bda0-1c0c70b1f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from transformers.cache_utils import *\n",
    "from fasta import * \n",
    "from typing import *\n",
    "\n",
    "\n",
    "class ModifiedLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper, with modifications.\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[\"Cache\"] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            if hasattr(past_key_value, \"get_key\") and hasattr(past_key_value, \"get_value\"):\n",
    "                previous_key = past_key_value.get_key(self.layer_idx)\n",
    "                previous_value = past_key_value.get_value(self.layer_idx)\n",
    "                if previous_key is not None:\n",
    "                    key_states = torch.cat([previous_key, key_states], dim=2)\n",
    "                if previous_value is not None:\n",
    "                    value_states = torch.cat([previous_value, value_states], dim=2)\n",
    "            else:\n",
    "                raise AttributeError(\"Cache object lacks required methods 'get_key' and 'get_value'.\")\n",
    "\n",
    "        key_states = key_states.transpose(-1, -2)\n",
    "\n",
    "        # Use fasta_attn for efficient computation\n",
    "        try:\n",
    "            attention_output = fasta_attn(\n",
    "                query_states.flatten(0, 1),  # Combine batch and head dimensions\n",
    "                key_states.flatten(0, 1),\n",
    "                block_size=128\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in fasta_attn: {e}\")\n",
    "\n",
    "        attention_output = attention_output.view(*input_shape, -1).contiguous()\n",
    "\n",
    "        # Weighted sum of value states\n",
    "        attn_output = torch.einsum(\"bhls,bhld->bhld\", attention_output, value_states)\n",
    "\n",
    "        # Merge heads and project back to hidden size\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(*input_shape, -1)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # Update cache\n",
    "        present_key_value = past_key_value or Cache()\n",
    "        present_key_value.add(key_states, value_states)\n",
    "\n",
    "        return attn_output, None, present_key_value\n",
    "\n",
    "\n",
    "# Replace LlamaAttention with ModifiedLlamaAttention in the model\n",
    "def replace_attention_modules(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and hasattr(module, \"q_proj\"):\n",
    "            parent_name = name.rsplit(\".\", 1)[0]\n",
    "            parent = dict(model.named_modules())[parent_name]\n",
    "            setattr(parent, name.split(\".\")[-1], ModifiedLlamaAttention(module.config, module.layer_idx))\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(model, tokenizer, text, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Move input tensors to GPU\n",
    "    model.to(device)  # Move model to GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    return perplexity.item()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Clone the model to create a modified version\n",
    "modified_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "replace_attention_modules(modified_model)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Measure original model performance\n",
    "start_time = time.time()\n",
    "original_perplexity = compute_perplexity(model, tokenizer, text, device)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "# Measure modified model performance\n",
    "start_time = time.time()\n",
    "modified_perplexity = compute_perplexity(modified_model, tokenizer, text, device)\n",
    "modified_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Perplexity: {original_perplexity}, Time: {original_time:.4f}s\")\n",
    "print(f\"Modified Perplexity: {modified_perplexity}, Time: {modified_time:.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
