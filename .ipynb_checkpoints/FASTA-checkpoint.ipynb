{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae37cd5-1bd9-4d8a-9db6-fe6c500ecccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA: Full Average Scaled Tiling Attention\n",
    "# implement a sparse attention using triton using the following methods\n",
    "# in the standard self attention, the attention weight is computed like this: attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "# assume a function:\n",
    "# def att_weight(Q,K_T):\n",
    "#    return Q@K_T\n",
    "# FASTA is a sparse approximation for the above function which works as follows:\n",
    "# the Q and K are divided into equal sized chunks\n",
    "# assume  QxK^T to be [Q0,Q1,....Qn-1]*[K0,K1,....Kn-1] where each of them are equal sized chunks from the initial embeddings.\n",
    "# in the full product if Q0*K0 then you do the regular multiplication, but if Q0*K1 or whenever the indices are not same, do avg(Q0)*avg(K1) and then broadcast this value in the shape of that grid.\n",
    "# create a triton kernel which implements the above operation if i==j then intra-index, if i!=j then inter-index\n",
    "# generate code and test case for the kernels first before proceeding to the full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9800f6e-b1bf-4d0a-a89d-33258d2c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard torch self-attention\n",
    "import torch\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b084ed-7c5f-4205-950b-251e05d70131",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d56918-2729-444e-adb2-058602cd5471",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 22:17:\n    # Compute row using floor division and then compute col without using tl.mod\n    row = tl.cast(tl.floor(tl.cast(pid, tl.float32) / tl.cast(N, tl.float32)), tl.int32)\n    col = pid - row * N\n\n    # Compute the base pointers for the current Q and K chunks\n    q_base = Q_ptr + row * BLOCK_SIZE * stride_q0\n    k_base = K_ptr + col * BLOCK_SIZE * stride_k0\n\n    # Initialize shared memory for Q and K chunks\n    # Shape: (BLOCK_SIZE, D)\n    Q_chunk = tl.load(\n        q_base + tl.arange(0, BLOCK_SIZE)[:, None] * stride_q0 + tl.arange(0, D)[None, :] * stride_q1,\n                 ^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/language/core.py:35\u001b[0m, in \u001b[0;36mbuiltin.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you forget to add @triton.jit ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`_builder` argument must be provided outside of JIT functions.)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/language/core.py:1192\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, end, _builder)\u001b[0m\n\u001b[1;32m   1191\u001b[0m end \u001b[38;5;241m=\u001b[39m _constexpr_to_value(end)\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m semantic\u001b[38;5;241m.\u001b[39marange(start, end, _builder)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/language/semantic.py:503\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, end, builder)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(end, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marange\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms arguments must be of type tl.constexpr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    504\u001b[0m is_start_int64 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(start \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: arange's arguments must be of type tl.constexpr",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 152\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Run the test case\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     test_fasta_attention()\n",
      "Cell \u001b[0;32mIn[14], line 125\u001b[0m, in \u001b[0;36mtest_fasta_attention\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(N \u001b[38;5;241m*\u001b[39m BLOCK_SIZE, D, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Compute attention using FASTA\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m attn_fasta \u001b[38;5;241m=\u001b[39m fasta_attention(Q, K, BLOCK_SIZE\u001b[38;5;241m=\u001b[39mBLOCK_SIZE)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Reference computation using PyTorch\u001b[39;00m\n\u001b[1;32m    128\u001b[0m attn_ref \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((N \u001b[38;5;241m*\u001b[39m BLOCK_SIZE, N \u001b[38;5;241m*\u001b[39m BLOCK_SIZE), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[14], line 99\u001b[0m, in \u001b[0;36mfasta_attention\u001b[0;34m(Q, K, BLOCK_SIZE)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Launch Triton kernel\u001b[39;00m\n\u001b[1;32m     98\u001b[0m grid \u001b[38;5;241m=\u001b[39m (N \u001b[38;5;241m*\u001b[39m N,)\n\u001b[0;32m---> 99\u001b[0m fasta_attn_kernel[grid](\n\u001b[1;32m    100\u001b[0m     Q, K, attn,\n\u001b[1;32m    101\u001b[0m     N, D, BLOCK_SIZE,\n\u001b[1;32m    102\u001b[0m     stride_q0, stride_q1,\n\u001b[1;32m    103\u001b[0m     stride_k0, stride_k1,\n\u001b[1;32m    104\u001b[0m     stride_attn0, stride_attn1,\n\u001b[1;32m    105\u001b[0m     num_warps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    106\u001b[0m     num_stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(grid\u001b[38;5;241m=\u001b[39mgrid, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    663\u001b[0m         src,\n\u001b[1;32m    664\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m    665\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/compiler/compiler.py:276\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    274\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     module \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mmake_ir(options, codegen_fns, context)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    278\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ast_to_ttir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m, context\u001b[38;5;241m=\u001b[39mcontext, options\u001b[38;5;241m=\u001b[39moptions, codegen_fns\u001b[38;5;241m=\u001b[39mcodegen_fns)\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 22:17:\n    # Compute row using floor division and then compute col without using tl.mod\n    row = tl.cast(tl.floor(tl.cast(pid, tl.float32) / tl.cast(N, tl.float32)), tl.int32)\n    col = pid - row * N\n\n    # Compute the base pointers for the current Q and K chunks\n    q_base = Q_ptr + row * BLOCK_SIZE * stride_q0\n    k_base = K_ptr + col * BLOCK_SIZE * stride_k0\n\n    # Initialize shared memory for Q and K chunks\n    # Shape: (BLOCK_SIZE, D)\n    Q_chunk = tl.load(\n        q_base + tl.arange(0, BLOCK_SIZE)[:, None] * stride_q0 + tl.arange(0, D)[None, :] * stride_q1,\n                 ^"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Triton Kernel for FASTA Attention\n",
    "@triton.jit\n",
    "def fasta_attn_kernel(\n",
    "    Q_ptr, K_ptr, attn_ptr,\n",
    "    N, D, BLOCK_SIZE,\n",
    "    stride_q0, stride_q1,\n",
    "    stride_k0, stride_k1,\n",
    "    stride_attn0, stride_attn1\n",
    "):\n",
    "    # Calculate program ID\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    # Compute row using floor division and then compute col without using tl.mod\n",
    "    row = tl.cast(tl.floor(tl.cast(pid, tl.float32) / tl.cast(N, tl.float32)), tl.int32)\n",
    "    col = pid - row * N\n",
    "    \n",
    "    # Compute the base pointers for the current Q and K chunks\n",
    "    q_base = Q_ptr + row * BLOCK_SIZE * stride_q0\n",
    "    k_base = K_ptr + col * BLOCK_SIZE * stride_k0\n",
    "    \n",
    "    # Initialize shared memory for Q and K chunks\n",
    "    # Shape: (BLOCK_SIZE, D)\n",
    "    Q_chunk = tl.load(\n",
    "        q_base + tl.arange(0, BLOCK_SIZE)[:, None] * stride_q0 + tl.arange(0, D)[None, :] * stride_q1,\n",
    "        mask=True, \n",
    "        other=0.0\n",
    "    )\n",
    "    K_chunk = tl.load(\n",
    "        k_base + tl.arange(0, BLOCK_SIZE)[:, None] * stride_k0 + tl.arange(0, D)[None, :] * stride_k1,\n",
    "        mask=True, \n",
    "        other=0.0\n",
    "    )\n",
    "    \n",
    "    if row == col:\n",
    "        # Intra-chunk multiplication: Q_chunk @ K_chunk^T\n",
    "        # Compute the dot product manually\n",
    "        acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "        for k in range(D):\n",
    "            q = Q_chunk[:, k]  # Shape: (BLOCK_SIZE,)\n",
    "            k_ = K_chunk[:, k]  # Shape: (BLOCK_SIZE,)\n",
    "            acc += q[:, None] * k_[None, :]\n",
    "        attn = acc\n",
    "    else:\n",
    "        # Inter-chunk average multiplication\n",
    "        # Compute the average of Q_chunk and K_chunk manually\n",
    "        Q_sum = tl.zeros((D,), dtype=tl.float32)\n",
    "        K_sum = tl.zeros((D,), dtype=tl.float32)\n",
    "        for i in range(BLOCK_SIZE):\n",
    "            Q_sum += Q_chunk[i]\n",
    "            K_sum += K_chunk[i]\n",
    "        Q_avg = Q_sum / BLOCK_SIZE\n",
    "        K_avg = K_sum / BLOCK_SIZE\n",
    "        # Compute the dot product of averages\n",
    "        attn_val = tl.dot(Q_avg, K_avg)\n",
    "        # Broadcast the scalar to a BLOCK_SIZE x BLOCK_SIZE matrix\n",
    "        attn = tl.broadcast(attn_val, (BLOCK_SIZE, BLOCK_SIZE))\n",
    "    \n",
    "    # Compute the base pointer for the current attention block\n",
    "    attn_base = attn_ptr + row * BLOCK_SIZE * stride_attn0 + col * BLOCK_SIZE * stride_attn1\n",
    "    \n",
    "    # Store the attention weights\n",
    "    tl.store(\n",
    "        attn_base + tl.arange(0, BLOCK_SIZE)[:, None] * stride_attn0 + tl.arange(0, BLOCK_SIZE)[None, :] * stride_attn1,\n",
    "        attn, \n",
    "        mask=True\n",
    "    )\n",
    "\n",
    "# Attention Function\n",
    "def fasta_attention(Q, K, BLOCK_SIZE=128):\n",
    "    \"\"\"\n",
    "    Computes FASTA attention using Triton.\n",
    "\n",
    "    Args:\n",
    "        Q (torch.Tensor): Query tensor of shape (N*BLOCK_SIZE, D)\n",
    "        K (torch.Tensor): Key tensor of shape (N*BLOCK_SIZE, D)\n",
    "        BLOCK_SIZE (int): Size of each chunk\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention weights of shape (N*BLOCK_SIZE, N*BLOCK_SIZE)\n",
    "    \"\"\"\n",
    "    assert Q.shape == K.shape, \"Q and K must have the same shape\"\n",
    "    total_size, D = Q.shape\n",
    "    N = total_size // BLOCK_SIZE\n",
    "    assert N * BLOCK_SIZE == total_size, \"Total size must be divisible by BLOCK_SIZE\"\n",
    "    \n",
    "    attn = torch.empty((total_size, total_size), device=Q.device, dtype=Q.dtype)\n",
    "    \n",
    "    # Define strides\n",
    "    stride_q0, stride_q1 = Q.stride()\n",
    "    stride_k0, stride_k1 = K.stride()\n",
    "    stride_attn0, stride_attn1 = attn.stride()\n",
    "    \n",
    "    # Launch Triton kernel\n",
    "    grid = (N * N,)\n",
    "    fasta_attn_kernel[grid](\n",
    "        Q, K, attn,\n",
    "        N, D, BLOCK_SIZE,\n",
    "        stride_q0, stride_q1,\n",
    "        stride_k0, stride_k1,\n",
    "        stride_attn0, stride_attn1,\n",
    "        num_warps=4,\n",
    "        num_stages=3\n",
    "    )\n",
    "    return attn\n",
    "\n",
    "def test_fasta_attention():\n",
    "    \"\"\"\n",
    "    Tests the FASTA attention implementation by comparing it against a reference PyTorch implementation.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    N = 4           # Number of chunks\n",
    "    BLOCK_SIZE = 16 # Chunk size\n",
    "    D = 32          # Dimension\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Initialize Q and K with random values\n",
    "    Q = torch.randn(N * BLOCK_SIZE, D, device='cuda', dtype=torch.float32)\n",
    "    K = torch.randn(N * BLOCK_SIZE, D, device='cuda', dtype=torch.float32)\n",
    "\n",
    "    # Compute attention using FASTA\n",
    "    attn_fasta = fasta_attention(Q, K, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    # Reference computation using PyTorch\n",
    "    attn_ref = torch.zeros((N * BLOCK_SIZE, N * BLOCK_SIZE), device='cuda', dtype=torch.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            Q_i = Q[i*BLOCK_SIZE:(i+1)*BLOCK_SIZE]\n",
    "            K_j = K[j*BLOCK_SIZE:(j+1)*BLOCK_SIZE]\n",
    "            if i == j:\n",
    "                # Intra-chunk multiplication\n",
    "                ref = torch.matmul(Q_i, K_j.T)\n",
    "            else:\n",
    "                # Inter-chunk average multiplication\n",
    "                Q_avg = Q_i.mean(dim=0, keepdim=True)\n",
    "                K_avg = K_j.mean(dim=0, keepdim=True)\n",
    "                ref = torch.matmul(Q_avg, K_avg.T).expand(BLOCK_SIZE, BLOCK_SIZE)\n",
    "            attn_ref[i*BLOCK_SIZE:(i+1)*BLOCK_SIZE, j*BLOCK_SIZE:(j+1)*BLOCK_SIZE] = ref\n",
    "\n",
    "    # Verify the results\n",
    "    if torch.allclose(attn_fasta, attn_ref, atol=1e-4):\n",
    "        print(\"Test passed! FASTA attention matches the reference implementation.\")\n",
    "    else:\n",
    "        max_diff = (attn_fasta - attn_ref).abs().max()\n",
    "        print(f\"Test failed! Maximum difference: {max_diff}\")\n",
    "\n",
    "# Run the test case\n",
    "if __name__ == \"__main__\":\n",
    "    test_fasta_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae66e6-7d90-4b02-b1ea-7e97933b851c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
