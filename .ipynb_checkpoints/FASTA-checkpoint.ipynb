{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75bccb1d-cecf-4582-9cd6-c8278af0d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA: Full Average Scaled Tiling Attention\n",
    "# implement a sparse attention using triton using the following methods\n",
    "# in the standard self attention, the attention weight is computed like this: attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "# assume a function:\n",
    "# def att_weight(Q,K_T):\n",
    "#    return Q@K_T\n",
    "# FASTA is a sparse approximation for the above function which works as follows:\n",
    "# def att_weight(Q,K_T,n_chunks):\n",
    "#    return Q@K_T # sparse approximation\n",
    "# the Q and K are divided into equal sized chunks\n",
    "# assume  QxK^T to be [Q0,Q1,....Qn-1]*[K0,K1,....Kn-1] where each of them are equal sized chunks from the initial embeddings.\n",
    "# in the full product if Q0*K0 then you do the regular multiplication, but if Q0*K1 or whenever the indices are not same, do avg(Q0)*avg(K1) and then broadcast this value in the shape of that grid.\n",
    "# create a triton kernel which implements the above operation if i==j then intra-index, if i!=j then inter-index\n",
    "# generate code and test case for the kernels first before proceeding to the full implementation\n",
    "# the overall time complexity should be O(n^2/c^2+n*d*c) where c is number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9800f6e-b1bf-4d0a-a89d-33258d2c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard torch self-attention\n",
    "import torch\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b084ed-7c5f-4205-950b-251e05d70131",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7d56918-2729-444e-adb2-058602cd5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: (64,)\n",
      "Maximum intra-block difference: 7.62939453125e-06\n",
      "Mean intra-block difference: 8.333481673616916e-07\n",
      "Maximum inter-block difference: 29.288122177124023\n",
      "Mean inter-block difference: 6.261228357042585\n",
      "Intra-block attention matches the reference within the acceptable tolerance.\n",
      "Inter-block attention has been approximated, resulting in differences as expected.\n",
      "Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def sparse_attn_kernel(\n",
    "    Q_ptr, K_ptr, attn_ptr,\n",
    "    N, D: tl.constexpr, BLOCK_SIZE: tl.constexpr,\n",
    "    stride_q0, stride_q1,\n",
    "    stride_k0, stride_k1,\n",
    "    stride_attn0, stride_attn1,\n",
    "):\n",
    "    # Get the program ID and compute row and column indices\n",
    "    pid = tl.program_id(0)\n",
    "    n_blocks = tl.cdiv(N, BLOCK_SIZE)\n",
    "    row_block_idx = pid // n_blocks\n",
    "    col_block_idx = pid % n_blocks\n",
    "    \n",
    "    # Calculate offsets\n",
    "    row_start = row_block_idx * BLOCK_SIZE\n",
    "    col_start = col_block_idx * BLOCK_SIZE\n",
    "    \n",
    "    # Create block pointers\n",
    "    offs_q = row_start + tl.arange(0, BLOCK_SIZE)\n",
    "    offs_k = col_start + tl.arange(0, BLOCK_SIZE)\n",
    "    offs_d = tl.arange(0, D)\n",
    "    \n",
    "    # Initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n",
    "    \n",
    "    # Load Q and K blocks\n",
    "    q_ptrs = Q_ptr + offs_q[:, None] * stride_q0 + offs_d[None, :] * stride_q1\n",
    "    k_ptrs = K_ptr + offs_k[:, None] * stride_k0 + offs_d[None, :] * stride_k1\n",
    "    \n",
    "    q_mask = offs_q[:, None] < N\n",
    "    k_mask = offs_k[:, None] < N\n",
    "    \n",
    "    q_block = tl.load(q_ptrs, mask=q_mask, other=0.0)\n",
    "    k_block = tl.load(k_ptrs, mask=k_mask, other=0.0)\n",
    "    \n",
    "    if row_block_idx == col_block_idx:\n",
    "        # Intra-block: compute full attention\n",
    "        for d in range(D):\n",
    "            # Corrected: Include row_start in Q offsets\n",
    "            q_offsets = (row_start + tl.arange(0, BLOCK_SIZE)) * stride_q0 + d * stride_q1\n",
    "            # Corrected: Include col_start in K offsets\n",
    "            k_offsets = (col_start + tl.arange(0, BLOCK_SIZE)) * stride_k0 + d * stride_k1\n",
    "            \n",
    "            # Load the d-th column into q_vals and k_vals\n",
    "            q_vals = tl.load(Q_ptr + q_offsets, mask=(tl.arange(0, BLOCK_SIZE) < N), other=0.0)\n",
    "            k_vals = tl.load(K_ptr + k_offsets, mask=(tl.arange(0, BLOCK_SIZE) < N), other=0.0)\n",
    "            \n",
    "            # Compute outer product and update accumulator\n",
    "            acc += q_vals[:, None] * k_vals[None, :]\n",
    "            \n",
    "    if row_block_idx != col_block_idx:\n",
    "        # Inter-block: compute average of entire Q and K blocks\n",
    "        sum_q = tl.sum(q_block, axis=1)  # Shape: (BLOCK_SIZE,)\n",
    "        sum_k = tl.sum(k_block, axis=1)  # Shape: (BLOCK_SIZE,)\n",
    "        \n",
    "        # Then, sum over BLOCK_SIZE to get a single scalar for each block\n",
    "        total_sum_q = tl.sum(sum_q)  # Scalar\n",
    "        total_sum_k = tl.sum(sum_k)  # Scalar\n",
    "        \n",
    "        # Compute the average\n",
    "        avg_q = total_sum_q / (D * BLOCK_SIZE)\n",
    "        avg_k = total_sum_k / (D * BLOCK_SIZE)\n",
    "        \n",
    "        # Compute the scalar outer product\n",
    "        outer = avg_q * avg_k  # Scalar\n",
    "        \n",
    "        # Broadcast the scalar to the entire BLOCK_SIZE x BLOCK_SIZE matrix\n",
    "        acc += outer    \n",
    "    # Store the results\n",
    "    offs_attn_i = row_start + tl.arange(0, BLOCK_SIZE)\n",
    "    offs_attn_j = col_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    attn_ptrs = attn_ptr + offs_attn_i[:, None] * stride_attn0 + offs_attn_j[None, :] * stride_attn1\n",
    "    mask = (offs_attn_i[:, None] < N) & (offs_attn_j[None, :] < N)\n",
    "    tl.store(attn_ptrs, acc, mask=mask)\n",
    "\n",
    "def get_attn_weight(Q, K, block_size=128):\n",
    "    \"\"\"\n",
    "    Computes FASTA attention using Triton.\n",
    "    \n",
    "    Args:\n",
    "        Q (torch.Tensor): Query tensor of shape (N, D)\n",
    "        K (torch.Tensor): Key tensor of shape (N, D)\n",
    "        block_size (int): Size of attention blocks\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Attention weights of shape (N, N)\n",
    "    \"\"\"\n",
    "    N, D = Q.shape\n",
    "    # Ensure tensors are contiguous\n",
    "    Q = Q.contiguous()\n",
    "    K = K.contiguous()\n",
    "    \n",
    "    # Create output tensor\n",
    "    attn = torch.empty((N, N), device=Q.device, dtype=Q.dtype)\n",
    "    \n",
    "    # Calculate grid size\n",
    "    n_blocks = triton.cdiv(N, block_size)\n",
    "    grid = (n_blocks * n_blocks,)\n",
    "    print(f\"grid: {grid}\")\n",
    "    # Launch kernel\n",
    "    sparse_attn_kernel[grid](\n",
    "        Q, K, attn,\n",
    "        N, D, block_size,\n",
    "        Q.stride(0), Q.stride(1),\n",
    "        K.stride(0), K.stride(1),\n",
    "        attn.stride(0), attn.stride(1),\n",
    "    )\n",
    "    \n",
    "    return attn\n",
    "\n",
    "def test_fasta_attention():\n",
    "    \"\"\"\n",
    "    Test function for FASTA attention implementation\n",
    "    \"\"\"\n",
    "    # Test parameters\n",
    "    N = 64  # Sequence length\n",
    "    D = 64   # Hidden dimension\n",
    "    block_size = 8\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # Generate random inputs\n",
    "    torch.manual_seed(0)\n",
    "    Q = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    K = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Compute attention using FASTA\n",
    "    attn_fasta = get_attn_weight(Q, K, block_size=block_size)\n",
    "    \n",
    "    # Compute reference attention\n",
    "    attn_ref = Q @ K.T\n",
    "    \n",
    "    # Calculate grid parameters\n",
    "    n_blocks = (N + block_size - 1) // block_size\n",
    "    \n",
    "    # Initialize difference trackers\n",
    "    max_diff_intra = 0.0\n",
    "    max_diff_inter = 0.0\n",
    "    mean_diff_intra = 0.0\n",
    "    mean_diff_inter = 0.0\n",
    "    \n",
    "    for i in range(n_blocks):\n",
    "        for j in range(n_blocks):\n",
    "            row_start = i * block_size\n",
    "            col_start = j * block_size\n",
    "            row_end = min(row_start + block_size, N)\n",
    "            col_end = min(col_start + block_size, N)\n",
    "            \n",
    "            fasta_block = attn_fasta[row_start:row_end, col_start:col_end]\n",
    "            ref_block = attn_ref[row_start:row_end, col_start:col_end]\n",
    "            \n",
    "            if i == j:\n",
    "                # Intra-block: should match closely\n",
    "                diff = (fasta_block - ref_block).abs()\n",
    "                block_max_diff = diff.max().item()\n",
    "                block_mean_diff = diff.mean().item()\n",
    "                max_diff_intra = max(max_diff_intra, block_max_diff)\n",
    "                mean_diff_intra += block_mean_diff\n",
    "            else:\n",
    "                # Inter-block: approximated\n",
    "                # Compute the difference between approximated and actual\n",
    "                # Since FASTA approximates the entire block with a scalar, compare each element\n",
    "                diff = (fasta_block - ref_block).abs()\n",
    "                block_max_diff = diff.max().item()\n",
    "                block_mean_diff = diff.mean().item()\n",
    "                max_diff_inter = max(max_diff_inter, block_max_diff)\n",
    "                mean_diff_inter += block_mean_diff\n",
    "    \n",
    "    # Average the mean differences across all blocks\n",
    "    mean_diff_intra /= n_blocks\n",
    "    mean_diff_inter /= (n_blocks * (n_blocks - 1))\n",
    "    \n",
    "    print(f\"Maximum intra-block difference: {max_diff_intra}\")\n",
    "    print(f\"Mean intra-block difference: {mean_diff_intra}\")\n",
    "    print(f\"Maximum inter-block difference: {max_diff_inter}\")\n",
    "    print(f\"Mean inter-block difference: {mean_diff_inter}\")\n",
    "    \n",
    "    # Intra-block differences should be very small (close to 0)\n",
    "    # Inter-block differences will be higher due to approximation\n",
    "    \n",
    "    # Optional: Visual verification (commented out)\n",
    "    # torch.set_printoptions(precision=4)\n",
    "    # print(\"Difference matrix:\\n\", attn_fasta - attn_ref)\n",
    "    \n",
    "    # Assertions for intra-block differences only\n",
    "    intra_block_tolerance = 1e-5  # Tight tolerance since intra-block is exact\n",
    "    assert max_diff_intra < intra_block_tolerance, f\"Intra-block differences exceed tolerance: {max_diff_intra}\"\n",
    "    \n",
    "    print(\"Intra-block attention matches the reference within the acceptable tolerance.\")\n",
    "    print(\"Inter-block attention has been approximated, resulting in differences as expected.\")\n",
    "    print(\"Test completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fasta_attention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca1f92-fcc5-42fd-8f49-14871daad69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
