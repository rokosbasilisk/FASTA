{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bccb1d-cecf-4582-9cd6-c8278af0d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA: Full Average Scaled Tiling Attention\n",
    "# implement a sparse attention using triton using the following methods\n",
    "# in the standard self attention, the attention weight is computed like this: attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "# assume a function:\n",
    "# def att_weight(Q,K_T):\n",
    "#    return Q@K_T\n",
    "# FASTA is a sparse approximation for the above function which works as follows:\n",
    "# def att_weight(Q,K_T,n_chunks):\n",
    "#    return Q@K_T # sparse approximation\n",
    "# the Q and K are divided into equal sized chunks\n",
    "# assume  QxK^T to be [Q0,Q1,....Qn-1]*[K0,K1,....Kn-1] where each of them are equal sized chunks from the initial embeddings.\n",
    "# in the full product if Q0*K0 then you do the regular multiplication, but if Q0*K1 or whenever the indices are not same, do avg(Q0)*avg(K1) and then broadcast this value in the shape of that grid.\n",
    "# create a triton kernel which implements the above operation if i==j then intra-index, if i!=j then inter-index\n",
    "# generate code and test case for the kernels first before proceeding to the full implementation\n",
    "# the overall time complexity should be O(n^2/c^2+n*d*c) where c is number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9800f6e-b1bf-4d0a-a89d-33258d2c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard torch self-attention\n",
    "import torch\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b084ed-7c5f-4205-950b-251e05d70131",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d56918-2729-444e-adb2-058602cd5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Warming up the GPU...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Conflicting meta-parameters: BLOCK_SIZE_K, GROUP_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_M. Make sure that you don't re-define auto-tuned symbols.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 402\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# Optional: Visualize a specific attention block\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# visualize_attention_blocks(attn_fasta, attn_ref, block_size=32, block_i=0, block_j=1)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# ------------------- Main Execution -------------------\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 402\u001b[0m     test_fasta_attention_gaussian_benchmark()\n",
      "Cell \u001b[0;32mIn[3], line 316\u001b[0m, in \u001b[0;36mtest_fasta_attention_gaussian_benchmark\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarming up the GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m--> 316\u001b[0m     attn_fasta \u001b[38;5;241m=\u001b[39m fasta_attention(Q, K, block_size\u001b[38;5;241m=\u001b[39mblock_size, sigma\u001b[38;5;241m=\u001b[39msigma)\n\u001b[1;32m    317\u001b[0m     attn_ref \u001b[38;5;241m=\u001b[39m standard_self_attention(Q, K)\n\u001b[1;32m    318\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 230\u001b[0m, in \u001b[0;36mfasta_attention\u001b[0;34m(Q, K, block_size, sigma)\u001b[0m\n\u001b[1;32m    228\u001b[0m     K_block_T \u001b[38;5;241m=\u001b[39m K_block\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Perform matmul and convert back to float32\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     matmul_result \u001b[38;5;241m=\u001b[39m matmul(Q_block, K_block_T)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    231\u001b[0m     attn[row_start:row_end, row_start:row_end] \u001b[38;5;241m=\u001b[39m matmul_result\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Compute inter-block attention using Triton kernel\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b, activation)\u001b[0m\n\u001b[1;32m     87\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((M, N), device\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     88\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m META: (triton\u001b[38;5;241m.\u001b[39mcdiv(M, META[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK_SIZE_M\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m triton\u001b[38;5;241m.\u001b[39mcdiv(N, META[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK_SIZE_N\u001b[39m\u001b[38;5;124m'\u001b[39m]), )\n\u001b[0;32m---> 89\u001b[0m matmul_kernel[grid](\n\u001b[1;32m     90\u001b[0m     a, b, c,\n\u001b[1;32m     91\u001b[0m     M, N, K,\n\u001b[1;32m     92\u001b[0m     a\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), a\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     93\u001b[0m     b\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), b\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     94\u001b[0m     c\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), c\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     95\u001b[0m     BLOCK_SIZE_M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, BLOCK_SIZE_N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, BLOCK_SIZE_K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     96\u001b[0m     GROUP_SIZE_M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     97\u001b[0m     ACTIVATION\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(grid\u001b[38;5;241m=\u001b[39mgrid, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/autotuner.py:156\u001b[0m, in \u001b[0;36mAutotuner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    155\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 156\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bench(\u001b[38;5;241m*\u001b[39margs, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    157\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/autotuner.py:156\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    155\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 156\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bench(\u001b[38;5;241m*\u001b[39margs, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    157\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/autotuner.py:103\u001b[0m, in \u001b[0;36mAutotuner._bench\u001b[0;34m(self, config, *args, **meta)\u001b[0m\n\u001b[1;32m    101\u001b[0m conflicts \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m&\u001b[39m config\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conflicts:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConflicting meta-parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(conflicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt re-define auto-tuned symbols.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# augment meta-parameters with tunable ones\u001b[39;00m\n\u001b[1;32m    106\u001b[0m current \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(meta, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mall_kwargs())\n",
      "\u001b[0;31mValueError\u001b[0m: Conflicting meta-parameters: BLOCK_SIZE_K, GROUP_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_M. Make sure that you don't re-define auto-tuned symbols."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# ------------------- Triton MatMul Kernel -------------------\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "        GROUP_SIZE_M: tl.constexpr,\n",
    "        ACTIVATION: tl.constexpr,\n",
    "    ):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + tl.arange(0, BLOCK_SIZE_K)[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n",
    "\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        a = tl.load(a_ptrs, mask=(offs_am[:, None] < M)[:, None] & (tl.arange(0, BLOCK_SIZE_K)[None, :] < K - k * BLOCK_SIZE_K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(tl.arange(0, BLOCK_SIZE_K)[:, None] < K - k * BLOCK_SIZE_K)[:, None] & (offs_bn[None, :] < N)[None, :], other=0.0)\n",
    "        accumulator += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = tl.where(accumulator >= 0, accumulator, 0.01 * accumulator)\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    c_ptrs = c_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n",
    "    mask = (offs_am[:, None] < M)[:, None] & (offs_bn[None, :] < N)[None, :]\n",
    "    tl.store(c_ptrs, c, mask=mask)\n",
    "\n",
    "def matmul(a, b, activation=\"\"):\n",
    "    \"\"\"\n",
    "    Convenience wrapper for the Triton matmul kernel.\n",
    "\n",
    "    Args:\n",
    "        a (torch.Tensor): Tensor of shape (M, K)\n",
    "        b (torch.Tensor): Tensor of shape (K, N)\n",
    "        activation (str): Activation function to apply ('leaky_relu' or '')\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Result of the matrix multiplication (M, N)\n",
    "    \"\"\"\n",
    "    M, K = a.shape\n",
    "    K_b, N = b.shape\n",
    "    assert K == K_b, \"Incompatible dimensions for matmul\"\n",
    "\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_SIZE_M=128, BLOCK_SIZE_N=256, BLOCK_SIZE_K=64,\n",
    "        GROUP_SIZE_M=8,\n",
    "        ACTIVATION=activation,\n",
    "    )\n",
    "    return c\n",
    "\n",
    "# ------------------- Triton Inter-Block Attention Kernel -------------------\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE': 32, 'GROUP_SIZE': 4}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE': 16, 'GROUP_SIZE': 2}, num_stages=4, num_warps=4),\n",
    "    ],\n",
    "    key=['BLOCK_SIZE'],\n",
    ")\n",
    "@triton.jit\n",
    "def inter_block_attn_kernel(\n",
    "    Q_ptr, K_ptr, attn_ptr,\n",
    "    N, D: tl.constexpr, BLOCK_SIZE: tl.constexpr,\n",
    "    sigma: tl.constexpr,\n",
    "    stride_q0, stride_q1,\n",
    "    stride_k0, stride_k1,\n",
    "    stride_attn0, stride_attn1,\n",
    "):\n",
    "    # Program ID corresponds to (row_block, col_block)\n",
    "    pid = tl.program_id(0)\n",
    "    n_blocks = tl.cdiv(N, BLOCK_SIZE)\n",
    "    row_block = pid // n_blocks\n",
    "    col_block = pid % n_blocks\n",
    "\n",
    "    # Skip intra-blocks\n",
    "    if row_block == col_block:\n",
    "        return\n",
    "\n",
    "    row_start = row_block * BLOCK_SIZE\n",
    "    col_start = col_block * BLOCK_SIZE\n",
    "\n",
    "    # Calculate offsets for Q and K\n",
    "    offs_q = row_start + tl.arange(0, BLOCK_SIZE)[:, None]  # Shape: (BLOCK_SIZE, 1)\n",
    "    offs_k = col_start + tl.arange(0, BLOCK_SIZE)[None, :]  # Shape: (1, BLOCK_SIZE)\n",
    "\n",
    "    # Masks\n",
    "    q_mask = offs_q < N  # Shape: (BLOCK_SIZE, 1)\n",
    "    k_mask = offs_k < N  # Shape: (1, BLOCK_SIZE)\n",
    "\n",
    "    # Broadcast masks to match Q and K blocks\n",
    "    q_mask_broadcast = q_mask & (tl.arange(0, D)[None, :] < D)  # Shape: (BLOCK_SIZE, D)\n",
    "    k_mask_broadcast = k_mask & (tl.arange(0, D)[None, :] < D)  # Shape: (BLOCK_SIZE, D)\n",
    "\n",
    "    # Load Q and K blocks\n",
    "    Q_block = tl.load(Q_ptr + offs_q * stride_q0 + tl.arange(0, D)[None, :] * stride_q1, mask=q_mask_broadcast, other=0.0)  # Shape: (BLOCK_SIZE, D)\n",
    "    K_block = tl.load(K_ptr + offs_k * stride_k0 + tl.arange(0, D)[None, :] * stride_k1, mask=k_mask_broadcast, other=0.0)  # Shape: (BLOCK_SIZE, D)\n",
    "\n",
    "    # Compute vector averages\n",
    "    avg_q = tl.sum(Q_block, axis=1) / D  # Shape: (BLOCK_SIZE,)\n",
    "    avg_k = tl.sum(K_block, axis=1) / D  # Shape: (BLOCK_SIZE,)\n",
    "\n",
    "    # Compute outer product\n",
    "    outer = avg_q[:, None] * avg_k[None, :]  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "    # Compute Gaussian weights\n",
    "    center = (BLOCK_SIZE - 1) / 2.0\n",
    "    i = tl.cast(tl.arange(0, BLOCK_SIZE)[:, None], tl.float32)  # Shape: (BLOCK_SIZE, 1)\n",
    "    j = tl.cast(tl.arange(0, BLOCK_SIZE)[None, :], tl.float32)  # Shape: (1, BLOCK_SIZE)\n",
    "    distance_sq = (i - center) * (i - center) + (j - center) * (j - center)  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    gaussian_weights = tl.exp(-distance_sq / (2.0 * sigma * sigma))  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    gaussian_weights /= tl.sum(gaussian_weights)  # Normalize\n",
    "\n",
    "    # Apply Gaussian weights\n",
    "    acc = outer * gaussian_weights  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "    # Calculate attention pointers\n",
    "    offs_attn_i = row_start + tl.arange(0, BLOCK_SIZE)[:, None]  # Shape: (BLOCK_SIZE, 1)\n",
    "    offs_attn_j = col_start + tl.arange(0, BLOCK_SIZE)[None, :]  # Shape: (1, BLOCK_SIZE)\n",
    "\n",
    "    attn_ptrs = attn_ptr + offs_attn_i * stride_attn0 + offs_attn_j * stride_attn1  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "    # Mask for boundaries\n",
    "    mask = (offs_attn_i < N) & (offs_attn_j < N)  # Shape: (BLOCK_SIZE, BLOCK_SIZE)\n",
    "\n",
    "    # Store the attention weights with masking\n",
    "    tl.store(attn_ptrs, acc, mask=mask)\n",
    "\n",
    "def compute_inter_block_attn(Q, K, attn, block_size=32, sigma=1.5):\n",
    "    \"\"\"\n",
    "    Computes inter-block attention using Triton kernel.\n",
    "\n",
    "    Args:\n",
    "        Q (torch.Tensor): Query tensor of shape (N, D)\n",
    "        K (torch.Tensor): Key tensor of shape (N, D)\n",
    "        attn (torch.Tensor): Attention tensor of shape (N, N) to store results\n",
    "        block_size (int): Size of attention blocks\n",
    "        sigma (float): Standard deviation for Gaussian spread\n",
    "    \"\"\"\n",
    "    N, D = Q.shape\n",
    "    n_blocks = triton.cdiv(N, block_size)\n",
    "    grid = (n_blocks * n_blocks, )\n",
    "\n",
    "    inter_block_attn_kernel[grid](\n",
    "        Q, K, attn,\n",
    "        N, D, block_size, sigma,\n",
    "        Q.stride(0), Q.stride(1),\n",
    "        K.stride(0), K.stride(1),\n",
    "        attn.stride(0), attn.stride(1),\n",
    "    )\n",
    "\n",
    "# ------------------- FASTA Attention Function -------------------\n",
    "\n",
    "def fasta_attention(Q, K, block_size=128, sigma=1.5):\n",
    "    \"\"\"\n",
    "    Computes FASTA attention using Triton for intra-blocks and Triton for inter-blocks.\n",
    "\n",
    "    Args:\n",
    "        Q (torch.Tensor): Query tensor of shape (N, D)\n",
    "        K (torch.Tensor): Key tensor of shape (N, D)\n",
    "        block_size (int): Size of attention blocks\n",
    "        sigma (float): Standard deviation for Gaussian spread\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention weights of shape (N, N)\n",
    "    \"\"\"\n",
    "    N, D = Q.shape\n",
    "    attn = torch.zeros((N, N), device=Q.device, dtype=torch.float32)\n",
    "\n",
    "    # Compute intra-block attention using Triton's matmul kernel\n",
    "    n_blocks = triton.cdiv(N, block_size)\n",
    "    for i in range(n_blocks):\n",
    "        row_start = i * block_size\n",
    "        row_end = min(row_start + block_size, N)\n",
    "        Q_block = Q[row_start:row_end].half()  # Convert to float16 for the matmul kernel\n",
    "        K_block = K[row_start:row_end].half()\n",
    "        # Transpose K_block for correct matmul dimensions\n",
    "        K_block_T = K_block.transpose(-2, -1)\n",
    "        # Perform matmul and convert back to float32\n",
    "        matmul_result = matmul(Q_block, K_block_T).float()\n",
    "        attn[row_start:row_end, row_start:row_end] = matmul_result\n",
    "\n",
    "    # Compute inter-block attention using Triton kernel\n",
    "    compute_inter_block_attn(Q.half(), K.half(), attn, block_size=block_size, sigma=sigma)\n",
    "\n",
    "    return attn\n",
    "\n",
    "# ------------------- Standard Self-Attention Function -------------------\n",
    "\n",
    "def standard_self_attention(Q, K):\n",
    "    \"\"\"\n",
    "    Computes standard self-attention using PyTorch's optimized matrix multiplication.\n",
    "\n",
    "    Args:\n",
    "        Q (torch.Tensor): Query tensor of shape (N, D)\n",
    "        K (torch.Tensor): Key tensor of shape (N, D)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention weights of shape (N, N)\n",
    "    \"\"\"\n",
    "    return Q @ K.T\n",
    "\n",
    "# ------------------- Visualization Function -------------------\n",
    "\n",
    "def visualize_attention_blocks(attn_fasta, attn_ref, block_size=32, block_i=0, block_j=1):\n",
    "    \"\"\"\n",
    "    Visualizes specific attention blocks for comparison.\n",
    "\n",
    "    Args:\n",
    "        attn_fasta (torch.Tensor): FASTA attention weights.\n",
    "        attn_ref (torch.Tensor): Reference (standard) attention weights.\n",
    "        block_size (int): Size of the attention block to visualize.\n",
    "        block_i (int): Block row index.\n",
    "        block_j (int): Block column index.\n",
    "    \"\"\"\n",
    "    row_start = block_i * block_size\n",
    "    col_start = block_j * block_size\n",
    "    row_end = min(row_start + block_size, attn_fasta.shape[0])\n",
    "    col_end = min(col_start + block_size, attn_fasta.shape[1])\n",
    "\n",
    "    fasta_block = attn_fasta[row_start:row_end, col_start:col_end].cpu().numpy()\n",
    "    ref_block = attn_ref[row_start:row_end, col_start:col_end].cpu().numpy()\n",
    "    diff_block = fasta_block - ref_block\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(ref_block, annot=False, fmt=\".2f\", cmap=\"viridis\")\n",
    "    plt.title(\"Reference Attention Block\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(fasta_block, annot=False, fmt=\".2f\", cmap=\"viridis\")\n",
    "    plt.title(\"FASTA Attention Block (Inter)\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(diff_block, annot=False, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Difference Block\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ------------------- Benchmarking Function -------------------\n",
    "\n",
    "def test_fasta_attention_gaussian_benchmark():\n",
    "    \"\"\"\n",
    "    Benchmark function for FASTA attention implementation using Gaussian-like spread\n",
    "    and standard self-attention. Runs each attention computation 100 times and\n",
    "    plots the time distributions.\n",
    "    \"\"\"\n",
    "    # Test parameters\n",
    "    N = 1024  # Sequence length\n",
    "    D = 128   # Hidden dimension\n",
    "    block_size = 128\n",
    "    sigma = 1.5  # Standard deviation for Gaussian spread\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Generate random inputs\n",
    "    torch.manual_seed(0)\n",
    "    Q = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    K = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # Warm-up runs to stabilize GPU performance\n",
    "    print(\"Warming up the GPU...\")\n",
    "    for _ in range(10):\n",
    "        attn_fasta = fasta_attention(Q, K, block_size=block_size, sigma=sigma)\n",
    "        attn_ref = standard_self_attention(Q, K)\n",
    "    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "    \n",
    "    # Number of benchmarking iterations\n",
    "    num_iterations = 100\n",
    "    \n",
    "    # Initialize lists to store timing data\n",
    "    fasta_times = []\n",
    "    standard_times = []\n",
    "    \n",
    "    print(\"Starting benchmarking...\")\n",
    "    # Use tqdm for progress visualization\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Benchmarking\"):\n",
    "        # Benchmark FASTA attention\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            start_fasta = torch.cuda.Event(enable_timing=True)\n",
    "            end_fasta = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start_fasta.record()\n",
    "            attn_fasta = fasta_attention(Q, K, block_size=block_size, sigma=sigma)\n",
    "            end_fasta.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_fasta = start_fasta.elapsed_time(end_fasta)  # Time in milliseconds\n",
    "        else:\n",
    "            import time\n",
    "            start_fasta = time.time()\n",
    "            attn_fasta = fasta_attention(Q, K, block_size=block_size, sigma=sigma)\n",
    "            end_fasta = time.time()\n",
    "            elapsed_fasta = (end_fasta - start_fasta) * 1000  # Convert to milliseconds\n",
    "\n",
    "        fasta_times.append(elapsed_fasta)\n",
    "\n",
    "        # Benchmark standard self-attention\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            start_std = torch.cuda.Event(enable_timing=True)\n",
    "            end_std = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "            start_std.record()\n",
    "            attn_ref = standard_self_attention(Q, K)\n",
    "            end_std.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_std = start_std.elapsed_time(end_std)  # Time in milliseconds\n",
    "        else:\n",
    "            import time\n",
    "            start_std = time.time()\n",
    "            attn_ref = standard_self_attention(Q, K)\n",
    "            end_std = time.time()\n",
    "            elapsed_std = (end_std - start_std) * 1000  # Convert to milliseconds\n",
    "\n",
    "        standard_times.append(elapsed_std)\n",
    "    \n",
    "    print(\"Benchmarking completed!\")\n",
    "    \n",
    "    # Convert timing lists to NumPy arrays for easier handling\n",
    "    fasta_times = torch.tensor(fasta_times).cpu().numpy()\n",
    "    standard_times = torch.tensor(standard_times).cpu().numpy()\n",
    "    \n",
    "    # Plotting the time distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sns.histplot(fasta_times, color='blue', label='FASTA Attention (Gaussian Spread)', kde=True, stat=\"density\", bins=50, alpha=0.6)\n",
    "    sns.histplot(standard_times, color='orange', label='Standard Self-Attention', kde=True, stat=\"density\", bins=50, alpha=0.6)\n",
    "    \n",
    "    plt.title('Time Distribution of FASTA vs Standard Self-Attention')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Optional: Print summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(f\"FASTA Attention - Mean: {fasta_times.mean():.4f} ms, Std: {fasta_times.std():.4f} ms\")\n",
    "    print(f\"Standard Self-Attention - Mean: {standard_times.mean():.4f} ms, Std: {standard_times.std():.4f} ms\")\n",
    "    \n",
    "    # Optional: Visualize a specific attention block\n",
    "    # visualize_attention_blocks(attn_fasta, attn_ref, block_size=32, block_i=0, block_j=1)\n",
    "\n",
    "# ------------------- Main Execution -------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fasta_attention_gaussian_benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9fde8-b1b9-48c0-b850-4c8843ba3c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
