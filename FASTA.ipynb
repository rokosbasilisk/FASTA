{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bccb1d-cecf-4582-9cd6-c8278af0d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTA: Full Average Scaled Tiling Attention\n",
    "# implement a sparse attention using triton using the following methods\n",
    "# in the standard self attention, the attention weight is computed like this: attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "# assume a function:\n",
    "# def att_weight(Q,K_T):\n",
    "#    return Q@K_T\n",
    "# FASTA is a sparse approximation for the above function which works as follows:\n",
    "# def att_weight(Q,K_T,n_chunks):\n",
    "#    return Q@K_T # sparse approximation\n",
    "# the Q and K are divided into equal sized chunks\n",
    "# assume  QxK^T to be [Q0,Q1,....Qn-1]*[K0,K1,....Kn-1] where each of them are equal sized chunks from the initial embeddings.\n",
    "# in the full product if Q0*K0 then you do the regular multiplication, but if Q0*K1 or whenever the indices are not same, do avg(Q0)*avg(K1) and then broadcast this value in the shape of that grid.\n",
    "# create a triton kernel which implements the above operation if i==j then intra-index, if i!=j then inter-index\n",
    "# generate code and test case for the kernels first before proceeding to the full implementation\n",
    "# the overall time complexity should be O(n^2/c^2+n*d*c) where c is number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b084ed-7c5f-4205-950b-251e05d70131",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3ac3a-f669-4cdd-8de8-66f1593af27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from fasta import fasta_attn  # Ensure this is the Triton kernel\n",
    "import numpy as np\n",
    "\n",
    "def test_fasta_attention_benchmark():\n",
    "    \"\"\"\n",
    "    Benchmark function for optimized FASTA attention implementation\n",
    "    and standard self-attention. Includes batching for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    # Test parameters\n",
    "    N = 4096  # Sequence length\n",
    "    D = 64     # Hidden dimension\n",
    "    block_size = 64\n",
    "    device = 'cuda'\n",
    "    num_iterations = 100  # Number of benchmarking iterations\n",
    "    \n",
    "    # Ensure CUDA is available\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available. Please run on a CUDA-enabled device.\"\n",
    "    \n",
    "    # Generate random inputs\n",
    "    torch.manual_seed(0)\n",
    "    Q = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "    K = torch.randn(N, D, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Reshape Q and K for FASTA attention\n",
    "    Q_fasta = Q.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, N, D)\n",
    "    K_fasta = K.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, N, D)\n",
    "\n",
    "    # Precompute all standard outputs\n",
    "    print(\"Computing standard self-attention for all iterations...\")\n",
    "    standard_times = []\n",
    "    standard_outputs = []\n",
    "\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Standard Self-Attention\"):\n",
    "        start_std = torch.cuda.Event(enable_timing=True)\n",
    "        end_std = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start_std.record()\n",
    "        attn_ref = Q @ K.T\n",
    "        end_std.record()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_std = start_std.elapsed_time(end_std)\n",
    "        standard_times.append(elapsed_std)\n",
    "        standard_outputs.append(attn_ref.detach().cpu().numpy())\n",
    "\n",
    "    print(\"Computing optimized FASTA attention for all iterations...\")\n",
    "    # Precompute all FASTA outputs\n",
    "    fasta_times = []\n",
    "    fasta_outputs = []\n",
    "\n",
    "    for _ in tqdm(range(num_iterations), desc=\"FASTA Attention\"):\n",
    "        start_fasta = torch.cuda.Event(enable_timing=True)\n",
    "        end_fasta = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start_fasta.record()\n",
    "        attn_fasta = fasta_attn(Q_fasta, K_fasta, block_size=block_size).squeeze(0).squeeze(0)\n",
    "        end_fasta.record()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_fasta = start_fasta.elapsed_time(end_fasta)\n",
    "        fasta_times.append(elapsed_fasta)\n",
    "        fasta_outputs.append(attn_fasta.detach().cpu().numpy())\n",
    "\n",
    "    # Convert timing lists to NumPy arrays\n",
    "    standard_times = np.array(standard_times)\n",
    "    fasta_times = np.array(fasta_times)\n",
    "\n",
    "    # Compute errors after all computations\n",
    "    print(\"Computing error metrics...\")\n",
    "    mae_list = []\n",
    "    max_error_list = []\n",
    "    relative_error_list = []\n",
    "\n",
    "    for ref, fasta in zip(standard_outputs, fasta_outputs):\n",
    "        abs_diff = np.abs(ref - fasta)\n",
    "        mae_list.append(abs_diff.mean().item())\n",
    "        max_error_list.append(abs_diff.max().item())\n",
    "        relative_error_list.append((abs_diff / np.abs(ref + 1e-6)).mean().item())\n",
    "\n",
    "    # Print results\n",
    "    print(\"Benchmarking completed!\")\n",
    "    print(\"\\nTiming Statistics:\")\n",
    "    print(f\"Standard Self-Attention - Mean: {standard_times.mean():.4f} ms, Std: {standard_times.std():.4f} ms\")\n",
    "    print(f\"FASTA Attention - Mean: {fasta_times.mean():.4f} ms, Std: {fasta_times.std():.4f} ms\")\n",
    "    print(f\"Percentage Improvement: {100 * (1 - fasta_times.mean() / standard_times.mean()):.2f}%\")\n",
    "\n",
    "    print(\"\\nError Metrics (Average over all iterations):\")\n",
    "    print(f\"Mean Absolute Error (MAE): {np.mean(mae_list):.6f}\")\n",
    "    print(f\"Maximum Absolute Error: {np.mean(max_error_list):.6f}\")\n",
    "    print(f\"Relative Error: {np.mean(relative_error_list):.6f}\")\n",
    "\n",
    "    # Plot timing distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(fasta_times, color='blue', label='FASTA Attention', kde=True, stat=\"density\", bins=50, alpha=0.6)\n",
    "    sns.histplot(standard_times, color='orange', label='Standard Attention', kde=True, stat=\"density\", bins=50, alpha=0.6)\n",
    "    plt.title('Timing Distributions')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fasta_attention_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
