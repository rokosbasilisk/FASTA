{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6366299-4240-4b6d-aa80-cd46bc24d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Instantiating ChunkedLlamaAttention without passing a `layer_idx` is not recommended and will lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` when creating this class.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'triton.language' has no attribute 'ones_like'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 210\u001b[0m\n\u001b[1;32m    208\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m    209\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(sequence_length))  \u001b[38;5;66;03m# e.g., 22 for N=512\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m benchmark_attention(\n\u001b[1;32m    211\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    212\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    213\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39msequence_length,\n\u001b[1;32m    214\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    215\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 195\u001b[0m, in \u001b[0;36mbenchmark_attention\u001b[0;34m(model_name, chunk_size, sequence_length, batch_size)\u001b[0m\n\u001b[1;32m    193\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[1;32m    196\u001b[0m chunked_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    197\u001b[0m chunked_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m    198\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mlogits[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m    199\u001b[0m     input_ids[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    200\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1164\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1165\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1166\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1167\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1168\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1169\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1170\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1171\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1172\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1173\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1175\u001b[0m )\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         position_embeddings,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m    915\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    916\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    917\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    918\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    919\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    920\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    921\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    641\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    643\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    644\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    645\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    646\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    647\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    648\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m, in \u001b[0;36mreplace_llamaattention_with_chunked_attention.<locals>.ChunkedLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    155\u001b[0m ):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked_attention(hidden_states, attention_mask, position_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m, in \u001b[0;36mChunkedAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m num_chunks \u001b[38;5;241m=\u001b[39m (seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[1;32m    116\u001b[0m grid \u001b[38;5;241m=\u001b[39m (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, num_chunks)\n\u001b[0;32m--> 117\u001b[0m chunk_attention_kernel[grid](\n\u001b[1;32m    118\u001b[0m     q, k, v, output,\n\u001b[1;32m    119\u001b[0m     q\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), q\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m2\u001b[39m), q\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    120\u001b[0m     k\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), k\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m2\u001b[39m), k\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    121\u001b[0m     v\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), v\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m2\u001b[39m), v\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    122\u001b[0m     output\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m), output\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m2\u001b[39m), output\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    123\u001b[0m     seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    124\u001b[0m     BLOCK_SIZE\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size),\n\u001b[1;32m    125\u001b[0m     HEAD_DIM\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(grid\u001b[38;5;241m=\u001b[39mgrid, warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:662\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 662\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    663\u001b[0m         src,\n\u001b[1;32m    664\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m    665\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/compiler/compiler.py:240\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# create cache manager\u001b[39;00m\n\u001b[1;32m    239\u001b[0m env_vars \u001b[38;5;241m=\u001b[39m get_cache_invalidating_env_vars()\n\u001b[0;32m--> 240\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtriton_key()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28msorted\u001b[39m(env_vars\u001b[38;5;241m.\u001b[39mitems()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256(key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    242\u001b[0m fn_cache_manager \u001b[38;5;241m=\u001b[39m get_cache_manager(\u001b[38;5;28mhash\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/compiler/compiler.py:109\u001b[0m, in \u001b[0;36mASTSource.hash\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Note - we stringify the keys here to allow sorting to work for cases\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# where constants have mixed int/str keys.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m sorted_constants \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m((\u001b[38;5;28mstr\u001b[39m(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m--> 109\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39mcache_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mhash()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msorted_sig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msorted_constants\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39msha256(key\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest()\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:758\u001b[0m, in \u001b[0;36mJITFunction.cache_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     dependencies_finder \u001b[38;5;241m=\u001b[39m DependenciesFinder(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m, src\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc)\n\u001b[0;32m--> 758\u001b[0m     dependencies_finder\u001b[38;5;241m.\u001b[39mvisit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse())\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhash \u001b[38;5;241m=\u001b[39m dependencies_finder\u001b[38;5;241m.\u001b[39mret \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarting_line_number)\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_global_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(dependencies_finder\u001b[38;5;241m.\u001b[39mused_global_vals\u001b[38;5;241m.\u001b[39mitems()))\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:426\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, AST):\n\u001b[0;32m--> 426\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:167\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_FunctionDef\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit_FunctionDef\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Save the local name, which may hide the global name.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_names \u001b[38;5;241m=\u001b[39m {arg\u001b[38;5;241m.\u001b[39marg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39margs}\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:426\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, AST):\n\u001b[0;32m--> 426\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:221\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Assign\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisitAssnTarget(node\u001b[38;5;241m.\u001b[39mtargets[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# This will re-visit the target, but that's OK.\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:428\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:134\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Call\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_triton_builtin(func) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    129\u001b[0m     func, JITFunction\n\u001b[1;32m    130\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Traverse arguments as well as node.func so we can find JITFunctions\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# passed to tl.reduce or tl.associative_scan as the combine_fn\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    135\u001b[0m     (func, ),\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit, node\u001b[38;5;241m.\u001b[39margs),\n\u001b[1;32m    137\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(kw\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mkeywords),\n\u001b[1;32m    138\u001b[0m ):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, JITFunction):\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:428\u001b[0m, in \u001b[0;36mNodeVisitor.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(item)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:127\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Call\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    124\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mstartswith(TRITON_MODULE)\n\u001b[0;32m--> 127\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(node\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_triton_builtin(func) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    129\u001b[0m     func, JITFunction\n\u001b[1;32m    130\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Traverse arguments as well as node.func so we can find JITFunctions\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# passed to tl.reduce or tl.associative_scan as the combine_fn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/ast.py:418\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    416\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    417\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node)\n",
      "File \u001b[0;32m~/anaconda3/envs/wip/lib/python3.11/site-packages/triton/runtime/jit.py:117\u001b[0m, in \u001b[0;36mDependenciesFinder.visit_Attribute\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(lhs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m TRITON_MODULE):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(lhs, node\u001b[38;5;241m.\u001b[39mattr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'triton.language' has no attribute 'ones_like'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "@triton.jit\n",
    "def chunk_attention_kernel(\n",
    "    Q, K, V, Output,\n",
    "    stride_qb, stride_qm, stride_qh,\n",
    "    stride_kb, stride_km, stride_kh,\n",
    "    stride_vb, stride_vm, stride_vh,\n",
    "    stride_ob, stride_om, stride_oh,\n",
    "    seq_len, chunk_size, head_dim,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "):\n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_head = tl.program_id(1)\n",
    "    pid_chunk = tl.program_id(2)\n",
    "    \n",
    "    chunk_start = pid_chunk * BLOCK_SIZE\n",
    "    offs_m = tl.arange(0, BLOCK_SIZE)\n",
    "    offs_n = tl.arange(0, HEAD_DIM)\n",
    "    mask = offs_m < (seq_len - chunk_start)\n",
    "    \n",
    "    q_base = Q + pid_batch * stride_qb + pid_head * stride_qh\n",
    "    k_base = K + pid_batch * stride_kb + pid_head * stride_kh\n",
    "    v_base = V + pid_batch * stride_vb + pid_head * stride_vh\n",
    "    \n",
    "    # Compute pointers for the current chunk\n",
    "    q_ptrs = q_base + chunk_start * stride_qm + offs_m[:, None] * stride_qm + offs_n[None, :] * 1\n",
    "    k_ptrs = k_base + chunk_start * stride_km + offs_m[:, None] * stride_km + offs_n[None, :] * 1\n",
    "    v_ptrs = v_base + chunk_start * stride_vm + offs_m[:, None] * stride_vm + offs_n[None, :] * 1\n",
    "    \n",
    "    q = tl.load(q_ptrs, mask=mask[:, None], other=0.0)\n",
    "    k = tl.load(k_ptrs, mask=mask[:, None], other=0.0)\n",
    "    v = tl.load(v_ptrs, mask=mask[:, None], other=0.0)\n",
    "    \n",
    "    # Intra-chunk attention\n",
    "    scores = tl.dot(q, tl.trans(k)) * (1.0 / tl.sqrt(tl.cast(HEAD_DIM, tl.float32)))\n",
    "    scores = tl.where(mask[:, None] & mask[None, :], scores, float(\"-inf\"))\n",
    "    \n",
    "    max_scores = tl.max(scores, axis=1)\n",
    "    scores = scores - max_scores[:, None]\n",
    "    exp_scores = tl.exp(scores)\n",
    "    sum_exp_scores = tl.sum(exp_scores, axis=1)\n",
    "    probs = exp_scores / sum_exp_scores[:, None]\n",
    "    \n",
    "    intra_output = tl.dot(probs, v)\n",
    "    \n",
    "    # Inter-chunk attention (mean of Q*K)\n",
    "    # Compute mean(Q) and mean(K) across the chunk\n",
    "    mean_q = tl.sum(q, axis=0) / BLOCK_SIZE\n",
    "    mean_k = tl.sum(k, axis=0) / BLOCK_SIZE\n",
    "    mean_scores = tl.dot(mean_q, tl.trans(mean_k)) * (1.0 / tl.sqrt(tl.cast(HEAD_DIM, tl.float32)))\n",
    "    \n",
    "    # Apply softmax to mean_scores\n",
    "    mean_scores = tl.where(tl.ones_like(mean_scores) > 0, mean_scores, float(\"-inf\"))  # Masking if needed\n",
    "    max_mean_scores = tl.max(mean_scores)\n",
    "    mean_scores = mean_scores - max_mean_scores\n",
    "    exp_mean_scores = tl.exp(mean_scores)\n",
    "    sum_exp_mean_scores = tl.sum(exp_mean_scores)\n",
    "    mean_probs = exp_mean_scores / sum_exp_mean_scores\n",
    "    \n",
    "    # Compute mean(V) across the chunk\n",
    "    mean_v = tl.sum(v, axis=0) / BLOCK_SIZE\n",
    "    inter_output = mean_probs * mean_v\n",
    "    \n",
    "    # Combine intra and inter chunk outputs\n",
    "    combined_output = intra_output + inter_output\n",
    "    \n",
    "    # Store the combined output\n",
    "    o_base = Output + pid_batch * stride_ob + pid_head * stride_oh\n",
    "    out_ptrs = o_base + chunk_start * stride_om + offs_m[:, None] * stride_om + offs_n[None, :] * 1\n",
    "    tl.store(out_ptrs, combined_output, mask=mask[:, None])\n",
    "\n",
    "class ChunkedAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, chunk_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.chunk_size = chunk_size\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Ensure divisibility\n",
    "        assert embed_dim % num_heads == 0, (\n",
    "            f\"Embedding dimension ({embed_dim}) must be divisible by the number of heads ({num_heads}).\"\n",
    "        )\n",
    "\n",
    "        self.q_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, position_ids=None, **kwargs):\n",
    "        batch_size, seq_len, embed_dim = hidden_states.shape\n",
    "\n",
    "        # Validate dimensions\n",
    "        assert embed_dim == self.embed_dim, (\n",
    "            f\"Input embedding dimension ({embed_dim}) does not match expected dimension ({self.embed_dim}).\"\n",
    "        )\n",
    "\n",
    "        q = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        k = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "        v = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "        output = torch.zeros_like(q)\n",
    "        num_chunks = (seq_len + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        grid = (batch_size, self.num_heads, num_chunks)\n",
    "        chunk_attention_kernel[grid](\n",
    "            q, k, v, output,\n",
    "            q.stride(0), q.stride(2), q.stride(1),\n",
    "            k.stride(0), k.stride(2), k.stride(1),\n",
    "            v.stride(0), v.stride(2), v.stride(1),\n",
    "            output.stride(0), output.stride(2), output.stride(1),\n",
    "            seq_len, self.chunk_size, self.head_dim,\n",
    "            BLOCK_SIZE=min(128, self.chunk_size),\n",
    "            HEAD_DIM=self.head_dim\n",
    "        )\n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        return self.out_proj(output)\n",
    "\n",
    "\n",
    "def replace_llamaattention_with_chunked_attention(model, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Replaces all instances of LlamaAttention in the model with ChunkedLlamaAttention.\n",
    "    \"\"\"\n",
    "    from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "\n",
    "    class ChunkedLlamaAttention(LlamaAttention):\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            self.chunked_attention = ChunkedAttention(\n",
    "                embed_dim=config.hidden_size,\n",
    "                num_heads=config.num_attention_heads,\n",
    "                chunk_size=chunk_size\n",
    "            )\n",
    "        \n",
    "        def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_ids=None,\n",
    "            past_key_value=None,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "            return self.chunked_attention(hidden_states, attention_mask, position_ids, **kwargs)\n",
    "\n",
    "    # Recursively replace LlamaAttention with ChunkedLlamaAttention\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LlamaAttention):\n",
    "            parent = model\n",
    "            components = name.split('.')\n",
    "            for comp in components[:-1]:\n",
    "                parent = getattr(parent, comp)\n",
    "            setattr(parent, components[-1], ChunkedLlamaAttention(model.config))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def benchmark_attention(model_name, chunk_size, sequence_length, batch_size):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval().cuda()\n",
    "\n",
    "    input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, sequence_length)).cuda()\n",
    "\n",
    "    # Standard Attention Benchmark\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    standard_time = time.time() - start_time\n",
    "    standard_loss = torch.nn.functional.cross_entropy(\n",
    "        outputs.logits[:, :-1].reshape(-1, outputs.logits.size(-1)),\n",
    "        input_ids[:, 1:].reshape(-1),\n",
    "    )\n",
    "\n",
    "    # Replace Attention Mechanism\n",
    "    model = replace_llamaattention_with_chunked_attention(model, chunk_size)\n",
    "    model.cuda()\n",
    "\n",
    "    # Chunked Attention Benchmark\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    chunked_time = time.time() - start_time\n",
    "    chunked_loss = torch.nn.functional.cross_entropy(\n",
    "        outputs.logits[:, :-1].reshape(-1, outputs.logits.size(-1)),\n",
    "        input_ids[:, 1:].reshape(-1),\n",
    "    )\n",
    "\n",
    "    print(f\"Standard: Time={standard_time:.3f}s, Perplexity={torch.exp(standard_loss):.3f}\")\n",
    "    print(f\"Chunked: Time={chunked_time:.3f}s, Perplexity={torch.exp(chunked_loss):.3f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Determine chunk size as sqrt(N)\n",
    "    sequence_length = 512\n",
    "    chunk_size = int(math.sqrt(sequence_length))  # e.g., 22 for N=512\n",
    "    benchmark_attention(\n",
    "        model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "        chunk_size=chunk_size,\n",
    "        sequence_length=sequence_length,\n",
    "        batch_size=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53397bec-87b5-440b-af27-024fa2843e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
