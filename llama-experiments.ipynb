{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a369b991-4d1c-4ea1-b5a5-3090d526e9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/oppenheimer/anaconda3/envs/wip/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "INFO:__main__:Replaced model.layers.0.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.1.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.2.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.3.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.4.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.5.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.6.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.7.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.8.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.9.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.10.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.11.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.12.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.13.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.14.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Replaced model.layers.15.self_attn with ModifiedLlamaAttention\n",
      "INFO:__main__:Attention module replacement complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape-ok: True attn_weights: 0.9348986148834229 attn_weights_ref: 0.9355298280715942 diff: -0.0006312288460321724\n",
      "shape-ok: True attn_weights: -5.912840843200684 attn_weights_ref: -5.916915416717529 diff: 0.004074214491993189\n",
      "shape-ok: True attn_weights: -4.427699089050293 attn_weights_ref: -4.430812835693359 diff: 0.003113734070211649\n",
      "shape-ok: True attn_weights: -3.5528852939605713 attn_weights_ref: -3.5554237365722656 diff: 0.0025387664791196585\n",
      "shape-ok: True attn_weights: -3.6028616428375244 attn_weights_ref: -3.6054270267486572 diff: 0.0025657552760094404\n",
      "shape-ok: True attn_weights: -3.718364715576172 attn_weights_ref: -3.7209603786468506 diff: 0.0025955475866794586\n",
      "shape-ok: True attn_weights: -3.3977255821228027 attn_weights_ref: -3.4001574516296387 diff: 0.0024314960464835167\n",
      "shape-ok: True attn_weights: -2.0609164237976074 attn_weights_ref: -2.062368869781494 diff: 0.0014524594880640507\n",
      "shape-ok: True attn_weights: -3.16814923286438 attn_weights_ref: -3.170318603515625 diff: 0.0021695008035749197\n",
      "shape-ok: True attn_weights: -3.342862129211426 attn_weights_ref: -3.3452036380767822 diff: 0.0023409973364323378\n",
      "shape-ok: True attn_weights: -4.216568946838379 attn_weights_ref: -4.219534873962402 diff: 0.0029665236361324787\n",
      "shape-ok: True attn_weights: -4.345800399780273 attn_weights_ref: -4.348813056945801 diff: 0.0030126310884952545\n",
      "shape-ok: True attn_weights: -4.628638744354248 attn_weights_ref: -4.631855487823486 diff: 0.0032166296150535345\n",
      "shape-ok: True attn_weights: -4.559841156005859 attn_weights_ref: -4.563080310821533 diff: 0.0032390672713518143\n",
      "shape-ok: True attn_weights: -4.012020111083984 attn_weights_ref: -4.014859676361084 diff: 0.002839804394170642\n",
      "shape-ok: True attn_weights: -4.38105583190918 attn_weights_ref: -4.3840718269348145 diff: 0.0030163808260113\n",
      "shape-ok: True attn_weights: 1.0156984329223633 attn_weights_ref: 1.0164285898208618 diff: -0.000730160332750529\n",
      "shape-ok: True attn_weights: -5.96556282043457 attn_weights_ref: -5.969675540924072 diff: 0.004113055765628815\n",
      "shape-ok: True attn_weights: -4.481753826141357 attn_weights_ref: -4.484987258911133 diff: 0.0032334376592189074\n",
      "shape-ok: True attn_weights: -3.672070026397705 attn_weights_ref: -3.6747076511383057 diff: 0.002637548604980111\n",
      "shape-ok: True attn_weights: -3.5912415981292725 attn_weights_ref: -3.593722105026245 diff: 0.0024806095752865076\n",
      "shape-ok: True attn_weights: -3.6883456707000732 attn_weights_ref: -3.690845012664795 diff: 0.002499456750229001\n",
      "shape-ok: True attn_weights: -3.20395827293396 attn_weights_ref: -3.206249237060547 diff: 0.002290980191901326\n",
      "shape-ok: True attn_weights: -1.9718526601791382 attn_weights_ref: -1.9732738733291626 diff: 0.0014212371315807104\n",
      "shape-ok: True attn_weights: -2.8709588050842285 attn_weights_ref: -2.8729567527770996 diff: 0.0019980762153863907\n",
      "shape-ok: True attn_weights: -3.119400978088379 attn_weights_ref: -3.1215524673461914 diff: 0.0021514601539820433\n",
      "shape-ok: True attn_weights: -4.0907769203186035 attn_weights_ref: -4.093587398529053 diff: 0.002810114761814475\n",
      "shape-ok: True attn_weights: -4.205470561981201 attn_weights_ref: -4.208380222320557 diff: 0.0029091271571815014\n",
      "shape-ok: True attn_weights: -4.409809112548828 attn_weights_ref: -4.4128899574279785 diff: 0.003081041155382991\n",
      "shape-ok: True attn_weights: -4.48832368850708 attn_weights_ref: -4.491544246673584 diff: 0.003220474347472191\n",
      "shape-ok: True attn_weights: -3.8079986572265625 attn_weights_ref: -3.8107151985168457 diff: 0.0027170695830136538\n",
      "shape-ok: True attn_weights: -4.241828441619873 attn_weights_ref: -4.244777202606201 diff: 0.0029489167500287294\n",
      "shape-ok: True attn_weights: 0.9979310035705566 attn_weights_ref: 0.998617947101593 diff: -0.0006869774661026895\n",
      "shape-ok: True attn_weights: -5.984756946563721 attn_weights_ref: -5.988943576812744 diff: 0.004186451900750399\n",
      "shape-ok: True attn_weights: -4.338399887084961 attn_weights_ref: -4.341463565826416 diff: 0.0030643953941762447\n",
      "shape-ok: True attn_weights: -3.574113368988037 attn_weights_ref: -3.5767321586608887 diff: 0.0026187703479081392\n",
      "shape-ok: True attn_weights: -3.6336190700531006 attn_weights_ref: -3.6361422538757324 diff: 0.0025234634522348642\n",
      "shape-ok: True attn_weights: -3.8134262561798096 attn_weights_ref: -3.8160340785980225 diff: 0.002607556525617838\n",
      "shape-ok: True attn_weights: -3.3671674728393555 attn_weights_ref: -3.3695337772369385 diff: 0.002366255037486553\n",
      "shape-ok: True attn_weights: -2.048417329788208 attn_weights_ref: -2.0498828887939453 diff: 0.0014653241960331798\n",
      "shape-ok: True attn_weights: -3.1299920082092285 attn_weights_ref: -3.132172107696533 diff: 0.0021803679410368204\n",
      "shape-ok: True attn_weights: -3.2542452812194824 attn_weights_ref: -3.2565245628356934 diff: 0.0022792734671384096\n",
      "shape-ok: True attn_weights: -4.1639180183410645 attn_weights_ref: -4.166823863983154 diff: 0.002906614914536476\n",
      "shape-ok: True attn_weights: -4.2996697425842285 attn_weights_ref: -4.302652359008789 diff: 0.002982412464916706\n",
      "shape-ok: True attn_weights: -4.404510021209717 attn_weights_ref: -4.407626628875732 diff: 0.0031169706489890814\n",
      "shape-ok: True attn_weights: -4.391198635101318 attn_weights_ref: -4.394349575042725 diff: 0.0031512517016381025\n",
      "shape-ok: True attn_weights: -3.781310796737671 attn_weights_ref: -3.7839515209198 diff: 0.0026408981066197157\n",
      "shape-ok: True attn_weights: -4.155351161956787 attn_weights_ref: -4.158196926116943 diff: 0.002845493610948324\n",
      "shape-ok: True attn_weights: 0.8876302242279053 attn_weights_ref: 0.8882446885108948 diff: -0.0006143604987300932\n",
      "shape-ok: True attn_weights: -5.949273586273193 attn_weights_ref: -5.953402042388916 diff: 0.004127917345613241\n",
      "shape-ok: True attn_weights: -4.542897701263428 attn_weights_ref: -4.5461249351501465 diff: 0.0032269712537527084\n",
      "shape-ok: True attn_weights: -3.757927179336548 attn_weights_ref: -3.760554552078247 diff: 0.0026272786781191826\n",
      "shape-ok: True attn_weights: -3.7592787742614746 attn_weights_ref: -3.76183819770813 diff: 0.0025598350912332535\n",
      "shape-ok: True attn_weights: -4.102194309234619 attn_weights_ref: -4.105024814605713 diff: 0.0028302522841840982\n",
      "shape-ok: True attn_weights: -3.645037889480591 attn_weights_ref: -3.6476194858551025 diff: 0.0025816713459789753\n",
      "shape-ok: True attn_weights: -2.262096881866455 attn_weights_ref: -2.2637007236480713 diff: 0.0016040001064538956\n",
      "shape-ok: True attn_weights: -3.3248119354248047 attn_weights_ref: -3.327103853225708 diff: 0.002291856799274683\n",
      "shape-ok: True attn_weights: -3.358642101287842 attn_weights_ref: -3.3609983921051025 diff: 0.002356649609282613\n",
      "shape-ok: True attn_weights: -4.526551246643066 attn_weights_ref: -4.52975606918335 diff: 0.0032050670124590397\n",
      "shape-ok: True attn_weights: -4.53287935256958 attn_weights_ref: -4.535974979400635 diff: 0.003095513442531228\n",
      "shape-ok: True attn_weights: -4.601656913757324 attn_weights_ref: -4.604818820953369 diff: 0.0031616126652806997\n",
      "shape-ok: True attn_weights: -4.425328731536865 attn_weights_ref: -4.428501605987549 diff: 0.0031720632687211037\n",
      "shape-ok: True attn_weights: -3.9562549591064453 attn_weights_ref: -3.9590675830841064 diff: 0.0028123450465500355\n",
      "shape-ok: True attn_weights: -4.306842803955078 attn_weights_ref: -4.309854030609131 diff: 0.003011096967384219\n",
      "shape-ok: True attn_weights: 0.9953461289405823 attn_weights_ref: 0.9960281848907471 diff: -0.000682103622239083\n",
      "shape-ok: True attn_weights: -5.822082996368408 attn_weights_ref: -5.826122760772705 diff: 0.004039910156279802\n",
      "shape-ok: True attn_weights: -4.1895904541015625 attn_weights_ref: -4.1925435066223145 diff: 0.0029528597369790077\n",
      "shape-ok: True attn_weights: -3.6299285888671875 attn_weights_ref: -3.632519483566284 diff: 0.0025907843373715878\n",
      "shape-ok: True attn_weights: -3.5669593811035156 attn_weights_ref: -3.569443464279175 diff: 0.002483983989804983\n",
      "shape-ok: True attn_weights: -3.6986324787139893 attn_weights_ref: -3.7012059688568115 diff: 0.0025733569636940956\n",
      "shape-ok: True attn_weights: -3.2691471576690674 attn_weights_ref: -3.27143931388855 diff: 0.002292033052071929\n",
      "shape-ok: True attn_weights: -1.8424739837646484 attn_weights_ref: -1.8437469005584717 diff: 0.0012729987502098083\n",
      "shape-ok: True attn_weights: -3.1280620098114014 attn_weights_ref: -3.1302411556243896 diff: 0.002179067814722657\n",
      "shape-ok: True attn_weights: -3.096518039703369 attn_weights_ref: -3.0986812114715576 diff: 0.0021629484836012125\n",
      "shape-ok: True attn_weights: -4.074113368988037 attn_weights_ref: -4.076937198638916 diff: 0.002824373310431838\n",
      "shape-ok: True attn_weights: -4.225259780883789 attn_weights_ref: -4.228175640106201 diff: 0.0029160946141928434\n",
      "shape-ok: True attn_weights: -4.4027099609375 attn_weights_ref: -4.405808448791504 diff: 0.0030983584001660347\n",
      "shape-ok: True attn_weights: -4.198496341705322 attn_weights_ref: -4.201488494873047 diff: 0.0029920702800154686\n",
      "shape-ok: True attn_weights: -3.650843381881714 attn_weights_ref: -3.653407335281372 diff: 0.0025640197563916445\n",
      "shape-ok: True attn_weights: -4.019022464752197 attn_weights_ref: -4.021870136260986 diff: 0.0028473364654928446\n",
      "shape-ok: True attn_weights: 0.8257821798324585 attn_weights_ref: 0.8263522982597351 diff: -0.0005700819310732186\n",
      "shape-ok: True attn_weights: -6.079721927642822 attn_weights_ref: -6.0839524269104 diff: 0.004230021964758635\n",
      "shape-ok: True attn_weights: -4.453352451324463 attn_weights_ref: -4.456533432006836 diff: 0.0031811038497835398\n",
      "shape-ok: True attn_weights: -3.7927067279815674 attn_weights_ref: -3.7953848838806152 diff: 0.0026786220259964466\n",
      "shape-ok: True attn_weights: -3.7920682430267334 attn_weights_ref: -3.7946865558624268 diff: 0.0026178983971476555\n",
      "shape-ok: True attn_weights: -4.023587226867676 attn_weights_ref: -4.0263824462890625 diff: 0.0027955989353358746\n",
      "shape-ok: True attn_weights: -3.5258851051330566 attn_weights_ref: -3.5283641815185547 diff: 0.002479138085618615\n",
      "shape-ok: True attn_weights: -2.1154096126556396 attn_weights_ref: -2.1168689727783203 diff: 0.0014594744425266981\n",
      "shape-ok: True attn_weights: -3.2627158164978027 attn_weights_ref: -3.2650067806243896 diff: 0.00229133409447968\n",
      "shape-ok: True attn_weights: -3.3448874950408936 attn_weights_ref: -3.347210645675659 diff: 0.0023232470266520977\n",
      "shape-ok: True attn_weights: -4.4985127449035645 attn_weights_ref: -4.501697063446045 diff: 0.003184062894433737\n",
      "shape-ok: True attn_weights: -4.508544921875 attn_weights_ref: -4.51167631149292 diff: 0.0031316031236201525\n",
      "shape-ok: True attn_weights: -4.6450371742248535 attn_weights_ref: -4.6482672691345215 diff: 0.0032299021258950233\n",
      "shape-ok: True attn_weights: -4.547328948974609 attn_weights_ref: -4.550629138946533 diff: 0.0032998514361679554\n",
      "shape-ok: True attn_weights: -3.971520185470581 attn_weights_ref: -3.9743034839630127 diff: 0.0027830253820866346\n",
      "shape-ok: True attn_weights: -4.254440784454346 attn_weights_ref: -4.257400035858154 diff: 0.0029587377794086933\n",
      "shape-ok: True attn_weights: 0.9360635876655579 attn_weights_ref: 0.9367010593414307 diff: -0.000637462071608752\n",
      "shape-ok: True attn_weights: -5.889026165008545 attn_weights_ref: -5.893106460571289 diff: 0.004079882521182299\n",
      "shape-ok: True attn_weights: -4.314781188964844 attn_weights_ref: -4.3178791999816895 diff: 0.0030977539718151093\n",
      "shape-ok: True attn_weights: -3.5533201694488525 attn_weights_ref: -3.555818796157837 diff: 0.0024980988819152117\n",
      "shape-ok: True attn_weights: -3.650675058364868 attn_weights_ref: -3.653163194656372 diff: 0.002488132333382964\n",
      "shape-ok: True attn_weights: -3.7380692958831787 attn_weights_ref: -3.7406675815582275 diff: 0.0025982405059039593\n",
      "shape-ok: True attn_weights: -3.250488758087158 attn_weights_ref: -3.252784252166748 diff: 0.002295765560120344\n",
      "shape-ok: True attn_weights: -2.1043450832366943 attn_weights_ref: -2.1058478355407715 diff: 0.001502829254604876\n",
      "shape-ok: True attn_weights: -3.0266427993774414 attn_weights_ref: -3.0287115573883057 diff: 0.002068763133138418\n",
      "shape-ok: True attn_weights: -3.242131471633911 attn_weights_ref: -3.244398355484009 diff: 0.002266999799758196\n",
      "shape-ok: True attn_weights: -4.272924900054932 attn_weights_ref: -4.27593994140625 diff: 0.003015821799635887\n",
      "shape-ok: True attn_weights: -4.2289509773254395 attn_weights_ref: -4.231922626495361 diff: 0.0029717155266553164\n",
      "shape-ok: True attn_weights: -4.549479961395264 attn_weights_ref: -4.552652359008789 diff: 0.003171885386109352\n",
      "shape-ok: True attn_weights: -4.430401802062988 attn_weights_ref: -4.433566570281982 diff: 0.00316478474996984\n",
      "shape-ok: True attn_weights: -3.82124662399292 attn_weights_ref: -3.823887825012207 diff: 0.002640853403136134\n",
      "shape-ok: True attn_weights: -4.188954830169678 attn_weights_ref: -4.191877841949463 diff: 0.002923351712524891\n",
      "shape-ok: True attn_weights: 0.9170148968696594 attn_weights_ref: 0.9176529049873352 diff: -0.0006380944396369159\n",
      "shape-ok: True attn_weights: -5.97689962387085 attn_weights_ref: -5.981054306030273 diff: 0.004154834430664778\n",
      "shape-ok: True attn_weights: -4.4591965675354 attn_weights_ref: -4.462405204772949 diff: 0.0032085259445011616\n",
      "shape-ok: True attn_weights: -3.692735433578491 attn_weights_ref: -3.6954212188720703 diff: 0.002685478888452053\n",
      "shape-ok: True attn_weights: -3.797635555267334 attn_weights_ref: -3.8002419471740723 diff: 0.0026059013325721025\n",
      "shape-ok: True attn_weights: -4.240929126739502 attn_weights_ref: -4.243911266326904 diff: 0.002981913508847356\n",
      "shape-ok: True attn_weights: -3.6454505920410156 attn_weights_ref: -3.6480040550231934 diff: 0.0025535414461046457\n",
      "shape-ok: True attn_weights: -2.4017364978790283 attn_weights_ref: -2.403404712677002 diff: 0.0016683294670656323\n",
      "shape-ok: True attn_weights: -3.5890417098999023 attn_weights_ref: -3.591527223587036 diff: 0.002485567471012473\n",
      "shape-ok: True attn_weights: -3.4992940425872803 attn_weights_ref: -3.5017738342285156 diff: 0.002479656832292676\n",
      "shape-ok: True attn_weights: -4.601024150848389 attn_weights_ref: -4.60424280166626 diff: 0.0032189541961997747\n",
      "shape-ok: True attn_weights: -4.607274055480957 attn_weights_ref: -4.610398769378662 diff: 0.0031252496410161257\n",
      "shape-ok: True attn_weights: -4.723364353179932 attn_weights_ref: -4.72661018371582 diff: 0.0032455953769385815\n",
      "shape-ok: True attn_weights: -4.4376678466796875 attn_weights_ref: -4.440827369689941 diff: 0.0031595309264957905\n",
      "shape-ok: True attn_weights: -3.963150978088379 attn_weights_ref: -3.9659621715545654 diff: 0.002811421174556017\n",
      "shape-ok: True attn_weights: -4.3064680099487305 attn_weights_ref: -4.309416770935059 diff: 0.0029488797299563885\n",
      "shape-ok: True attn_weights: 1.0210951566696167 attn_weights_ref: 1.0218074321746826 diff: -0.0007121602538973093\n",
      "shape-ok: True attn_weights: -5.951904773712158 attn_weights_ref: -5.956082344055176 diff: 0.004178029950708151\n",
      "shape-ok: True attn_weights: -4.442874908447266 attn_weights_ref: -4.446016788482666 diff: 0.0031416527926921844\n",
      "shape-ok: True attn_weights: -3.763909339904785 attn_weights_ref: -3.766589879989624 diff: 0.002680622274056077\n",
      "shape-ok: True attn_weights: -3.6495702266693115 attn_weights_ref: -3.6520607471466064 diff: 0.0024903607554733753\n",
      "shape-ok: True attn_weights: -3.7835209369659424 attn_weights_ref: -3.7861132621765137 diff: 0.002592099132016301\n",
      "shape-ok: True attn_weights: -3.3748795986175537 attn_weights_ref: -3.377275228500366 diff: 0.002395399147644639\n",
      "shape-ok: True attn_weights: -1.9824258089065552 attn_weights_ref: -1.9838253259658813 diff: 0.0013997523346915841\n",
      "shape-ok: True attn_weights: -3.2554359436035156 attn_weights_ref: -3.2577342987060547 diff: 0.002298125997185707\n",
      "shape-ok: True attn_weights: -3.3729584217071533 attn_weights_ref: -3.3753151893615723 diff: 0.0023570682387799025\n",
      "shape-ok: True attn_weights: -4.2549214363098145 attn_weights_ref: -4.257869243621826 diff: 0.0029478769283741713\n",
      "shape-ok: True attn_weights: -4.492419242858887 attn_weights_ref: -4.495508193969727 diff: 0.003088840516284108\n",
      "shape-ok: True attn_weights: -4.531617164611816 attn_weights_ref: -4.534726619720459 diff: 0.003109336830675602\n",
      "shape-ok: True attn_weights: -4.4943952560424805 attn_weights_ref: -4.497611045837402 diff: 0.00321566266939044\n",
      "shape-ok: True attn_weights: -3.9931724071502686 attn_weights_ref: -3.9959850311279297 diff: 0.0028126996476203203\n",
      "shape-ok: True attn_weights: -4.365658283233643 attn_weights_ref: -4.368637561798096 diff: 0.0029795560985803604\n",
      "shape-ok: True attn_weights: 0.8773345351219177 attn_weights_ref: 0.8779571056365967 diff: -0.0006224755779840052\n",
      "shape-ok: True attn_weights: -5.945720672607422 attn_weights_ref: -5.949863910675049 diff: 0.0041438862681388855\n",
      "shape-ok: True attn_weights: -4.403463363647461 attn_weights_ref: -4.406567573547363 diff: 0.0031043170019984245\n",
      "shape-ok: True attn_weights: -3.572831392288208 attn_weights_ref: -3.575392484664917 diff: 0.002560892840847373\n",
      "shape-ok: True attn_weights: -3.64044451713562 attn_weights_ref: -3.642961025238037 diff: 0.002516511594876647\n",
      "shape-ok: True attn_weights: -3.936047315597534 attn_weights_ref: -3.9387991428375244 diff: 0.0027519785799086094\n",
      "shape-ok: True attn_weights: -3.534205913543701 attn_weights_ref: -3.5366768836975098 diff: 0.002471109153702855\n",
      "shape-ok: True attn_weights: -2.140932559967041 attn_weights_ref: -2.1424338817596436 diff: 0.0015014495002105832\n",
      "shape-ok: True attn_weights: -3.140444040298462 attn_weights_ref: -3.1426544189453125 diff: 0.0022105183452367783\n",
      "shape-ok: True attn_weights: -3.1741323471069336 attn_weights_ref: -3.1763525009155273 diff: 0.0022199824452400208\n",
      "shape-ok: True attn_weights: -4.096783638000488 attn_weights_ref: -4.099581718444824 diff: 0.0027980178128927946\n",
      "shape-ok: True attn_weights: -4.172926902770996 attn_weights_ref: -4.175814151763916 diff: 0.002887354465201497\n",
      "shape-ok: True attn_weights: -4.447540283203125 attn_weights_ref: -4.450613021850586 diff: 0.0030729619320482016\n",
      "shape-ok: True attn_weights: -4.269084930419922 attn_weights_ref: -4.272190093994141 diff: 0.003105579875409603\n",
      "shape-ok: True attn_weights: -3.6800787448883057 attn_weights_ref: -3.6826202869415283 diff: 0.0025414302945137024\n",
      "shape-ok: True attn_weights: -4.073946952819824 attn_weights_ref: -4.076742172241211 diff: 0.0027952580712735653\n",
      "shape-ok: True attn_weights: 2.412121057510376 attn_weights_ref: 2.413834571838379 diff: -0.00171347102150321\n",
      "shape-ok: True attn_weights: -3.1428728103637695 attn_weights_ref: -3.1451141834259033 diff: 0.00224136165343225\n",
      "shape-ok: True attn_weights: -1.775270938873291 attn_weights_ref: -1.7763975858688354 diff: 0.0011266556102782488\n",
      "shape-ok: True attn_weights: -1.0054614543914795 attn_weights_ref: -1.006195306777954 diff: 0.00073398114182055\n",
      "shape-ok: True attn_weights: -1.3214882612228394 attn_weights_ref: -1.3222960233688354 diff: 0.0008077889797277749\n",
      "shape-ok: True attn_weights: -1.4414862394332886 attn_weights_ref: -1.4424680471420288 diff: 0.000981801305897534\n",
      "shape-ok: True attn_weights: -1.472213625907898 attn_weights_ref: -1.4732156991958618 diff: 0.0010020432528108358\n",
      "shape-ok: True attn_weights: -0.8328010439872742 attn_weights_ref: -0.8333966135978699 diff: 0.0005955075612291694\n",
      "shape-ok: True attn_weights: -1.5887380838394165 attn_weights_ref: -1.589949369430542 diff: 0.0012111880350857973\n",
      "shape-ok: True attn_weights: -1.2451905012130737 attn_weights_ref: -1.2460194826126099 diff: 0.0008289073593914509\n",
      "shape-ok: True attn_weights: -1.6430176496505737 attn_weights_ref: -1.6442011594772339 diff: 0.001183624961413443\n",
      "shape-ok: True attn_weights: -1.9706059694290161 attn_weights_ref: -1.971920371055603 diff: 0.0013144062831997871\n",
      "shape-ok: True attn_weights: -2.035125255584717 attn_weights_ref: -2.0365724563598633 diff: 0.001446941983886063\n",
      "shape-ok: True attn_weights: -1.8546324968338013 attn_weights_ref: -1.8559606075286865 diff: 0.0013279576087370515\n",
      "shape-ok: True attn_weights: -1.848731279373169 attn_weights_ref: -1.8500603437423706 diff: 0.0013290602946653962\n",
      "shape-ok: True attn_weights: -2.028801679611206 attn_weights_ref: -2.030374526977539 diff: 0.001572808250784874\n",
      "Original Perplexity: 34.13900192690955, Time: 7.6818s Output: Tags -actoreted Problem\n",
      "\n",
      "\n",
      " Predict a Trainingancing a Chat Model\n",
      "\n",
      " Pyized Mechanism\n",
      "\n",
      "This are given \n",
      "Modified Perplexity: 34.14089623338767, Time: 5.5122s Output: Tags -actoreted Problem\n",
      "\n",
      "\n",
      " Predict a Trainingancing a Chat Model\n",
      "\n",
      " Pyized Mechanism\n",
      "\n",
      "This are given \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from fasta import *\n",
    "from typing import *\n",
    "import logging\n",
    "\n",
    "class ModifiedLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        \n",
    "        attn_weights = fasta_attn(query_states, key_states, 128)/math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "def replace_attention_modules(model):\n",
    "    \"\"\"\n",
    "    Replaces the attention modules in the given model with ModifiedLlamaAttention modules,\n",
    "    ensuring all relevant parameters and attributes are copied.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model whose attention modules will be replaced.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and hasattr(module, \"q_proj\"):\n",
    "            parent_name = name.rsplit(\".\", 1)[0]\n",
    "            parent = dict(model.named_modules()).get(parent_name, None)\n",
    "\n",
    "            if parent is None:\n",
    "                logger.warning(f\"Parent module not found for {name}, skipping replacement.\")\n",
    "                continue\n",
    "\n",
    "            # Instantiate ModifiedLlamaAttention and copy weights\n",
    "            modified_attention = ModifiedLlamaAttention(module.config, module.layer_idx)\n",
    "\n",
    "            # Copy weights and biases for q_proj\n",
    "            modified_attention.q_proj.weight.data.copy_(module.q_proj.weight.data)\n",
    "            if module.q_proj.bias is not None:\n",
    "                modified_attention.q_proj.bias.data.copy_(module.q_proj.bias.data)\n",
    "\n",
    "            # Copy weights and biases for k_proj\n",
    "            modified_attention.k_proj.weight.data.copy_(module.k_proj.weight.data)\n",
    "            if module.k_proj.bias is not None:\n",
    "                modified_attention.k_proj.bias.data.copy_(module.k_proj.bias.data)\n",
    "\n",
    "            # Copy weights and biases for v_proj\n",
    "            modified_attention.v_proj.weight.data.copy_(module.v_proj.weight.data)\n",
    "            if module.v_proj.bias is not None:\n",
    "                modified_attention.v_proj.bias.data.copy_(module.v_proj.bias.data)\n",
    "\n",
    "            # Copy weights and biases for o_proj\n",
    "            modified_attention.o_proj.weight.data.copy_(module.o_proj.weight.data)\n",
    "            if module.o_proj.bias is not None:\n",
    "                modified_attention.o_proj.bias.data.copy_(module.o_proj.bias.data)\n",
    "\n",
    "            # Preserve additional attributes, if any\n",
    "            for attr_name in dir(module):\n",
    "                if not attr_name.startswith(\"_\") and not hasattr(modified_attention, attr_name):\n",
    "                    setattr(modified_attention, attr_name, getattr(module, attr_name))\n",
    "\n",
    "            # Replace the module and log the replacement\n",
    "            setattr(parent, name.split(\".\")[-1], modified_attention)\n",
    "            logger.info(f\"Replaced {name} with ModifiedLlamaAttention\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear unused memory after replacement\n",
    "    logger.info(\"Attention module replacement complete.\")\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for text using batching to reduce memory overhead,\n",
    "    and returns the generated output text.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        text (str): The input text.\n",
    "        device (torch.device): The device to run the computation on.\n",
    "        max_length (int): Maximum sequence length for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (perplexity, generated_text)\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_chunks = 0\n",
    "    generated_text = \"\"\n",
    "\n",
    "    for i in range(0, len(text), max_length):\n",
    "        chunk = text[i:i + max_length]\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass with labels to compute loss\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_chunks += 1\n",
    "\n",
    "            # Decode generated text\n",
    "            logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)  # Get the token IDs with highest probability\n",
    "            generated_chunk = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "            generated_text += generated_chunk + \" \"\n",
    "\n",
    "    model.to(\"cpu\")  # Move model back to CPU after computation\n",
    "    torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "    avg_loss = total_loss / num_chunks\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return perplexity, generated_text.strip()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample text\n",
    "text = open(\"sample_data.txt\", \"r\").read()\n",
    "\n",
    "# Measure original model performance\n",
    "start_time = time.time()\n",
    "original_perplexity, output = compute_perplexity(model, tokenizer, text, device)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "replace_attention_modules(model)\n",
    "\n",
    "start_time = time.time()\n",
    "modified_perplexity, output_modified = compute_perplexity(model, tokenizer, text, device)\n",
    "modified_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Perplexity: {original_perplexity}, Time: {original_time:.4f}s Output: {output[:100]}\")\n",
    "print(f\"Modified Perplexity: {modified_perplexity}, Time: {modified_time:.4f}s Output: {output_modified[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74814a-920d-4587-9338-00e8429c6ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
