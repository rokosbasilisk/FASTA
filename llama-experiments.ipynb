{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a369b991-4d1c-4ea1-b5a5-3090d526e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "from fasta import *\n",
    "from typing import *\n",
    "\n",
    "class ModifiedLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
    "\n",
    "        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
    "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        attn_weights = fasta_attn(query_states, key_states.transpose(2, 3),128) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:  # no matter the length, we just slice it\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "# Replace LlamaAttention with ModifiedLlamaAttention in the model\n",
    "def replace_attention_modules(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and hasattr(module, \"q_proj\"):\n",
    "            # Get parent module and replace the attention module\n",
    "            parent_name = name.rsplit(\".\", 1)[0]\n",
    "            parent = dict(model.named_modules())[parent_name]\n",
    "\n",
    "            # Instantiate ModifiedLlamaAttention and copy weights\n",
    "            modified_attention = ModifiedLlamaAttention(module.config, module.layer_idx)\n",
    "\n",
    "            # Copy weights and biases for q_proj\n",
    "            modified_attention.q_proj.weight.data = module.q_proj.weight.data.clone()\n",
    "            if module.q_proj.bias is not None:\n",
    "                modified_attention.q_proj.bias.data = module.q_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for k_proj\n",
    "            modified_attention.k_proj.weight.data = module.k_proj.weight.data.clone()\n",
    "            if module.k_proj.bias is not None:\n",
    "                modified_attention.k_proj.bias.data = module.k_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for v_proj\n",
    "            modified_attention.v_proj.weight.data = module.v_proj.weight.data.clone()\n",
    "            if module.v_proj.bias is not None:\n",
    "                modified_attention.v_proj.bias.data = module.v_proj.bias.data.clone()\n",
    "\n",
    "            # Copy weights and biases for o_proj\n",
    "            modified_attention.o_proj.weight.data = module.o_proj.weight.data.clone()\n",
    "            if module.o_proj.bias is not None:\n",
    "                modified_attention.o_proj.bias.data = module.o_proj.bias.data.clone()\n",
    "\n",
    "            # Replace the module and delete the original\n",
    "            setattr(parent, name.split(\".\")[-1], modified_attention)\n",
    "            del module  # Explicitly delete the old module to free memory\n",
    "\n",
    "            print(f\"Replaced {name} with ModifiedLlamaAttention\")\n",
    "    torch.cuda.empty_cache()  # Clear any unused memory\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Computes perplexity for text using batching to reduce memory overhead,\n",
    "    and returns the generated output text.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for the model.\n",
    "        text (str): The input text.\n",
    "        device (torch.device): The device to run the computation on.\n",
    "        max_length (int): Maximum sequence length for each chunk.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (perplexity, generated_text)\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to GPU\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_chunks = 0\n",
    "    generated_text = \"\"\n",
    "\n",
    "    for i in range(0, len(text), max_length):\n",
    "        chunk = text[i:i + max_length]\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass with labels to compute loss\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_chunks += 1\n",
    "\n",
    "            # Decode generated text\n",
    "            logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)  # Get the token IDs with highest probability\n",
    "            generated_chunk = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "            generated_text += generated_chunk + \" \"\n",
    "\n",
    "    model.to(\"cpu\")  # Move model back to CPU after computation\n",
    "    torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "    avg_loss = total_loss / num_chunks\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return perplexity, generated_text.strip()\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Sample text\n",
    "text = open(\"sample_data.txt\",\"r\").read()\n",
    "# Measure original model performance\n",
    "start_time = time.time()\n",
    "original_perplexity,output = compute_perplexity(model, tokenizer, text, device)\n",
    "original_time = time.time() - start_time\n",
    "\n",
    "replace_attention_modules(model)\n",
    "\n",
    "start_time = time.time()\n",
    "modified_perplexity,output_modified = compute_perplexity(model, tokenizer, text, device)\n",
    "modified_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Perplexity: {original_perplexity}, Time: {original_time:.4f}s Output: {output[:100]}\")\n",
    "print(f\"Modified Perplexity: {modified_perplexity}, Time: {modified_time:.4f}s Output: {output_modified[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025b873-1199-4580-80fe-d72cb58046c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
